---
title: "Introduction to HomeWork"
author: "Guohui Jiang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to HomeWork}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Overview

__StatComp22072__ is a simple R package developed to display all previous homework
for the 'Statistical Computing' course. The R package 'knitr','xtable','ggplot2','boot','bootstrap','scor','DAAG','MASS','VGAM','multilevel','bda','Rcpp' and 'microbenchmark' are used for homework so I would like to express my heartfelt thanks to their founders. The homework will be introduced in order as follows.

## 2022-09-09

## Question
1.Go through “R for Beginners” if you are not familiar with R programming.

## Answer
Have already finished reading “R for Beginners”.

&nbsp;

## Question
2.Use knitr to produce at least 3 examples (texts, figures, tables).


## Answer
(1). Use knitr to produce at an example of texts.

**Getting the job you want**. Employers form lasting impressions on the basis of
what they see and know about a job candidate. The first items that a prospective
employer is likely to see are your resume and application letter. If they are well written, they’ll make a good first impression and help you get the job.

&nbsp;

(2). Use knitr to produce at an example of figures.

One of R's own dataset named **mtcars** is used to draw the following basic scatter figure. The horizontal axis is the variable **mpg** and the vertical axis is the variable **hp**.

```{r}
plot(mtcars$mpg, mtcars$hp,type="p",main ="The scatter plot about mpg and hp")
```

**Conclusion**: It is not difficult to see that these two variables are negatively correlated.

&nbsp;

(3). Use knitr to produce at an example of tables.

Firstly, the **kable** function in the **knitr** package can be used to generate tables. The first six rows of R's own dataset named **pressure** are used to generate a simple table as follows.

```{r}
knitr::kable(head(pressure))
```

Secondly, the **xtable** function in the **xtable** package can be used to generate tables. The first six rows of R's own dataset named **pressure** are used to generate a simple table as follows.


```{r}
xtable::xtable(head(pressure))
```
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & temperature & pressure \\ 
  \hline
  1 & 0.00 & 0.00 \\ 
  2 & 20.00 & 0.00 \\ 
  3 & 40.00 & 0.01 \\ 
  4 & 60.00 & 0.03 \\ 
  5 & 80.00 & 0.09 \\ 
  6 & 100.00 & 0.27 \\ 
   \hline
\end{tabular}
\end{table}

**Conclusion**: It is not difficult to see that these two variables are positively correlated, namely the higher the temperature, the greater the pressure.


## 2022-09-16

## Question
**Ex 3.3.** The Pareto $\small (a,b)$ distribution has cdf $$\small F(x)=1-\displaystyle\left(\frac{x}{b}\right)^a, \qquad \quad x \geq b >0,a>0.$$
Firstly, derive the probability inverse transformation $\small F^{-1}(U)$. Secondly, use the inverse transform method to simulate a random sample from the Pareto $\small (2,2)$ distribution. Thirdly, graph the density histogram of the sample with the Pareto $\small (2,2)$ density superimposed for comparison.


## Answer
Three steps to solve this problem are shown as follows.

**Step1:** The expression of inverse probability transform is as follows
$$\small  F^{-1}(U)=\displaystyle\frac{b}{(1-u)^{1/a}}.$$

**Step2:** The inverse transform method used to simulate a random sample from the Pareto $\small (2,2)$ distribution is as follows.
```{r}
set.seed(12134)    #For repeatable results
n <- 1000          #Number of samples to generate
u <- runif(n)      #Generate n uniformly distributed random numbers
b <-2              #Value of the parameter b in Pareto distribution
a <-2              #Value of the parameter a in Pareto distribution
x <- b/(1-u)^(1/a) #The random sample which generated by the inverse transform method
m <-50             #Total data for display
x[1:m]             #Output the first m values of X vector
```

**Step3:** The density histogram used to compare the empirical and theoretical distributions is as follows.
```{r}
hist(x, prob = TRUE, main = expression(f(x) == 8/(x^3)))  
#Generate histogram with x as data source and add the title
y <- seq(0, 100, 0.001)   
#Generate a group of numbers y, starting from 2 and ending at 100. The interval between two numbers is 0.001
lines(y, 8/(y^3)) 
#Draw a curve on the histogram with the expression 8/(y ^ 3)
```

**Conclusion**: From the above figure, it is not difficult to see that the sample distribution can fit the theoretical distribution very well, so the generated random sample is effective.

&nbsp;


## Question
**Ex 3.7.**  Write a function to generate a random sample of size n from the Beta$\small (a,b)$ distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the Beta$\small (3,2)$ distribution. Graph the histogram of the sample with the theoretical Beta$\small (3,2)$ density superimposed.


## Answer
**Step1:** The probability density function of the distribution beta (3,2) is shown as follows
$$
\small\begin{equation}
\begin{split}
f_{X}(x)&= \frac{\mathrm{d} F(x) }{\mathrm{d} x} \\
&= d \left(\frac{x^2(1-x)}{\int_{0}^{1}x^2(1-x)dx}\right)/dx \\
&= 12x^2(1-x).
\end{split}
\end{equation}
$$
The value range of the function $f(x)$ is determined by its derivative as follows. Let
$$ \small\frac{\mathrm{d} f(x) }{\mathrm{d} x}= 12(2x-3x^2)=0,\quad x \in [0,1].$$
The range of function $\small(x)$ is $\small[0,\frac{4}{27}]$. Then Let $\small c=\frac{4}{27}$ and $\small c=12$, the acceptance-rejection algorithm with the highest and lowest efficiency theoretically will be both tested in the next two step.

**Step2:**  **The highest efficiency**. Let $\small g(x)=1,0<x<1$ and $\small c=\frac{4}{27}$. Then the function $\small \rho(x)$ which used to generate a random sample is written as follows.

$$
\small \rho(x)=\frac{f(x)}{cg(x)}= \frac{12x^2(1-x)}{\frac{16}{9}}=\frac{27}{4}x^2(1-x). 
$$
The code used to generate the random sample are as follows.

```{r}
set.seed(12134) #For repeatable results
n <- 1000       #Number of samples to generate
j<-k<- 0
y <- numeric(n)
while (k < n) {
  u <- runif(1)
  j <- j + 1
  x <- runif(1) #random variate from g(.)
  if (((27/4)*(x^2)*(1-x)) > u) {
    #accept x
    k <- k + 1
    y[k] <- x
  }
}
j # the number of experiments for generating n random numbers 
```


**Step3:**  **The lowest efficiency**. Let $\small g(x)=1,0<x<1$ and $\small c=12$. Then the function $\small \rho_1(x)$ which used to generate a random sample is written as follows.

$$
\small \rho_1(x)=\frac{f(x)}{cg(x)}= \frac{12x^2(1-x)}{12}=x^2(1-x). 
$$
The code used to generate the random sample are as follows.

```{r}
set.seed(12134) #For repeatable results
n1 <- 1000       #Number of samples to generate
j1<-k1<- 0
y1 <- numeric(n1)
while (k1 < n1) {
  u1 <- runif(1)
  j1 <- j1 + 1
  x1 <- runif(1) #random variate from g(.)
  if (((x1^2)*(1-x1)) > u1) {
    #accept x
    k1 <- k1 + 1
    y1[k1] <- x1
  }
}
j1 # the number of experiments for generating n random numbers 
```

**Step4:** The density histogram is used to compare the empirical and theoretical distributions as follows. 
```{r}
hist(y, prob = TRUE, main = expression(f(y) == 12*y^2*(1-y)))  
#Generate histogram with x as data source and add the title
x <- seq(0, 100, 0.001)  
#generate a group of numbers y, starting from 2 and ending at 100. The interval between two numbers is 0.001
lines(x, (12*(x^2)*(1-x))) 
#Draw a curve on the histogram with the expression 8/(y ^ 3)
```

**Step4:** Simliarly, the Q-Q plot can also be used to compare the empirical and theoretical distributions as follows. 

```{r}
qqplot(y,rbeta(n,3,2),xlab='Accpetance-rejection',ylab='rbeta')
abline(0,1,col='blue',lwd=2)
```

**Conclusion**: (1) From the results of step 2 and step 3,it is not difficult to see that the difference between the efficiency of this two acceptance-rejection methods is more than seven times; (2) From the above figures, it is not difficult to see that the sample distribution can fit the theoretical distribution very well, so the generated random numbers by the acceptance-rejection method are effective.


## Question
**Ex 3.12.**  Simulate a continuous Exponential-Gamma mixture. Suppose that the rate parameter $\small \Lambda$ has Gamma $\small (r,\beta)$ distribution and $\small Y$ has Exp$\small (\Lambda)$ distribution. That is, $\small (Y|\Lambda=\lambda) \sim f_Y(y|\lambda)=\lambda e^{-\lambda y}.$  Generate 1000 random observations from this mixture with $\small r=4$ and $\small \beta=2$.


## Answer
**Step1:** The marginal distribution of random variable Y is calculated as follows.
$$
\small\begin{equation}
\begin{split}
f_{Y}(y)&=\int_{0}^{+\infty}f(y,\lambda)d\lambda\\
&=\int_{0}^{+\infty}f_{Y}(y|\lambda)f_\Lambda(\lambda)d\lambda\\
&=\int_{0}^{+\infty}\frac{\beta^r}{\Gamma(r)}\lambda^{r-1} e^{-\lambda\beta} \lambda e^{-\lambda y} d\lambda \\
&=\frac{\beta^r}{\Gamma(r)}\int_{0}^{+\infty}\lambda^r e^{-\lambda(\beta+y)} d\lambda \\
&=\frac{\beta^r}{\Gamma(r)} \frac{r}{\beta+y} \int_{0}^{+\infty}\lambda^{r-1} e^{-\lambda(\beta+y)} d\lambda \\
&=\cdots \\
&=\frac{\beta^r}{\Gamma(r)} \frac{\Gamma(r+1)}{(\beta+y)^{r+1}} \\
&=\frac{r\beta^{r}}{(y+\beta)^{r+1}}.
\end{split}
\end{equation}
$$
By calculating the cumulative distribution function of random variable Y
$$\small F_{Y}(y)=1-\displaystyle\left(\frac{\beta}{\beta+y}\right)^{r}, \quad y \geq 0.$$ It can be seen that Y has a Pareto distribution.


**Step2:** The transformation methods is used for generating 1000 random observations y from this exponential-Gamma mixture distribution as follows

```{r}
set.seed(12134)  #For repeatable results   
n <- 1000        #Number of samples to generate
r <- 4           #Value of the parameter r in Gamma distribution 
beta <- 2        #Value of the parameter beta in Gamma distribution 
lambda <- rgamma(n, r, beta)  
#Generate n random numbers from Gamma distribution 
y <- rexp(n, lambda) 
#Generate n random numbers from exponential distribution conditioned on the lamdba before.
m <-50             #Total data for display
y[1:m]             #Output the first m values of X vector
```


&nbsp;


## Question
**Ex 3.13.** It can be shown that the mixture in Exercise 3.12 has a Pareto distribution with cdf $$\small F(y)=1-\displaystyle\left(\frac{\beta}{\beta+y}\right)^{\gamma}, \quad y \geq 0.$$ (This is an alternative parameterization of the Pareto cdf given in Exercise 3.3.) Generate 1000 random observations from the mixture with $\small \gamma=4$ and $\small \beta=2$. Compare the empirical and theoretical (Pareto) distributions by graphing the density histogram of the sample and superimposing the Pareto density curve.


## Answer
**Step1:** The transformation methods is used for generating 1000 random observations y from this exponential-Gamma mixture distribution as follows.

```{r}
set.seed(12134)  #For repeatable results   
n <- 1000        #Number of samples to generate
r <- 4           #Value of the parameter r in Gamma distribution 
beta <- 2        #Value of the parameter beta in Gamma distribution 
lambda <- rgamma(n, r, beta)  
#Generate n random numbers from Gamma distribution 
y <- rexp(n, lambda) 
#Generate n random numbers from exponential distribution conditioned on the lamdba before.
m <-50             #Total data for display
y[1:m]             #Output the first m values of X vector
```

**Step2:** The density histogram is used to compare the empirical and theoretical distributions as follows. 
```{r}
hist(y, prob = TRUE, main = expression(f(y)))  
#Generate histogram with x as data source and add the title
x <- seq(0, 10, 0.01)   
#Generate a group of numbers y, starting from 2 and ending at 100. The interval between two numbers is 0.001
lines(x, 32/(x+2)^5) 
#Draw a curve on the histogram with the expression 8/(y ^ 3)
```

**Conclusion**:  From the above figure, it is not difficult to see that the sample distribution can fit the theoretical distribution very well, so the generated random numbers are effective.

## 2022-09-23

## Question

1. Exercise 5.0 (Page 7, Chap3 ppt-Monte Carlo Integration).

2. Exercise 5.6 (Page 150, Statistical Computing with R).

3. Exercise 5.7 (Page 150, Statistical Computing with R). 

## Answer

### Exercise 5.0

**Problem.** Please complete the following series of exercises about fast sorting algorithm.

(1). Apply the fast sorting algorithm to randomly permuted numbers of $\small 1,\cdots,n$, while $\small n = 10^4, 2 \times 10^4, 4 \times 10^4, 6 \times 10^4, 8 \times 10^4$.

(2). Calculate computation time averaged over 100 simulations, denoted by $\small a_n$.

(3). Regress $\small a_n$ on $\small t_n:=n \log(n)$ and graphically show the results (scatter plot and regression line).

**Solution.** The following two parts will be used to solve this problem.

**Part I.** Let us qiuckly review the main contents for the fast sort algorithm briefly as folllows.

*Step1:* Randomly pick one number named $\small x_{\eta}$ and compare it
with all other numbers, place the smaller ones to the left of $\small x_{\eta}$ and the larger ones to the right of $\small x_{\eta}$.

*Step2:* Apply Step 1 to the left numbers and the right numbers
separately. Repeat such procedure recursively until $\small x_1, \cdots, x_n$ are placed in an increasing order as $\small x_{(1)}, \cdots, x_{(n)}$.

**Part II.** The three problems will be solved in order as follows.

(1). The variable **testt** $\small (i=1,\cdots,5)$ will be established to generate random samples with different values of n. A vector named **shijiann** will be used to record the elapsed time of the five tests. The code is shown as follows.

```{r}
## fast sorting algorithm
quick_sort<-function(x){
  num<-length(x)
  if(num==0||num==1){return(x)
  }else{
    a<-x[1]
    y<-x[-1]
    lower<-y[y<a]
    upper<-y[y>=a]
    return(c(quick_sort(lower),a,quick_sort(upper)))}
}

n1 <- c(1e4,2e4,4e4,6e4,8e4) #value of n

testt <- numeric(0)    #a vector for storing random sample
shijiann <- numeric(0)  #a vector for storing elapsed time
for (i in 1:5){ 
  testt <- sample(1:n1[i])     
  shijiann[i] <- system.time(quick_sort(testt))[1]
}
print(shijiann) 
```

(2). A **dual-cycle** will be used to randomly sample 100 times for every n and calculate the average elapsed time named **a_n**. The variable **test** will be used to store the sampling results and its dimension changes with the change of n. The variable **shijian** is used to store the elapsed time of each program and it is a matrix with 5 rows and 100 columns. The code is shown as follows.(**The dual-cycle will take about two minutes to run, please wait patiently.**)

```{r} 
n1 <- c(1e4,2e4,4e4,6e4,8e4) #value of n
cycle1 <- 100                #Number of cycles
shijian <- matrix(nrow=length(n1),ncol=cycle1) # a martix for storing elapsed time
a_n <- numeric(length(n1))  # a vector for storing average elapsed time
for (i in 1:length(n1)){
  test <- matrix(nrow=n1[i],ncol=cycle1) # a variable matrix for storing random sample
  for (j in 1:cycle1){ 
    test[,j] <- sample(1:n1[i])  #Store the random sample by column
    shijian[i,j] <- system.time(quick_sort(test[,j]))[1]
    
  }
  a_n[i] <- mean(shijian[i,])
}
print(a_n) 
```


(3). **Plot function** will be used to plot scatter-regression diagram with $\small t_n$ as horizontal axis and $\small a_n$ as longitudinal axis. The code is shown as follows.
```{r fig.align="center"}
t_n = (n1)*log(n1)  #horizontal axis
plot(x=t_n,y=a_n,type='o',main="The regression plot for t_n and a_n")  
```

**Conclusion.** From the scatter-regression diagram, it is not difficult to see that the higher the value of n, the longer the average elapsed time of the program.


### Exercise 5.6

**Problem.**  In Example 5.7 the control variate approach was illustrated for Monte Carlo integration of $$\small \theta=\int_{0}^{1}e^x dx.$$
Now consider the antithetic variate approach. Compute $\small Cov(e^U,e^{1-U})$ and $\small Var(e^U+e^{1-U})$, where $\small U \sim U(0,1)$. What is the percent reduction in variance of $\small \hat{\theta}$ that can be achieved using antithetic variates (compared with simple MC)?

**Solution.** The following three steps will be used to solve this problem.

**Step 1.** Compute $\small Cov(e^U,e^{1-U})$ where $\small U \sim U(0,1)$. $\small U$ and $\small 1-U$ are identically distributed.
$$\small \begin{equation}
\begin{split}
Cov(e^{U},e^{1-U})&=E(e^{U}e^{1-U})-E(e^{U})E(e^{1-U}) \\
&=\int_{0}^{1} e^{U}e^{1-U} dU-\int_{0}^{1} e^{U}dU \int_{0}^{1}e^{1-U} dU \\
&=e-(e-1)^2 \\
&=-0.2342106.
\end{split}
\end{equation}
$$

**Step 2.** Suppose $\small \hat{\theta_1}$ is the simple MC estimator and $\small \hat{\theta_2}$ is the antithetic estimator.Then if $\small U$ and $\small V$ are i.i.d. Uniform(0,1) variables, we will have

$$\begin{equation}
\begin{split}
\small Var(\hat{\theta_1})&=\small Var\Big(\frac{1}{2}(e^{U}+e^{V})\Big) \\
&= \small \frac{1}{4} \times 2 \times Var(e^{U})\\
&= \small \frac{1}{2} \Big[E(e^{2U})-E^2(e^U)\Big] \\
&= \small \frac{1}{2} \Big[\int_{0}^{1} e^{2U} dU-\Big(\int_{0}^{1} e^U dU\Big)^2\Big] \\
&= \small \frac{1}{2} \Big[\frac{1}{2}(e^2-1)-(e-1)^2\Big] \\
&= \small 0.1210178.
\end{split}
\end{equation}
$$

If antithetic variables are used, $\small e^U$ and $\small e^{1-U}$ are negatively correlated, therefore we have

$$\begin{equation}
\begin{split}
\small Var(\hat{\theta_2})&=\small Var\Big(\frac{1}{2}(e^{U}+e^{1-U})\Big) \\
&= \small \frac{1}{4} (Var(e^{U})+Var(e^{1-U})+2Cov(e^U,e^{1-U})) \\
&= \small \frac{1}{2} \Big(\frac{1}{2}(e^2-1)-(e-1)^2+e-(e-1)^2\Big) \\
&= \small 0.003912497.
\end{split}
\end{equation}
$$

**Step 3.** Compute the percent reduction in variance.
$$\begin{equation}
\begin{split}
\small \frac{|Var(\hat{\theta_1})-Var(\hat{\theta_2})|}{Var(\hat{\theta_1})} \times 100\% &=\small \frac{|0.1210178-0.003912497|}{0.1210178}\times 100\% \\
&=\small 96.767\%.
\end{split}
\end{equation}
$$
**Conclusion.** From the results, it is not difficult to see that the variance decreases by 96.767% after using antithetic variables.

### Exercise 5.7

**Problem.** Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\small \theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

**Solution.** The code which is used to compute an empirical estimate of the percent reduction in variance using the antithetic variate is shown as follows.

```{r}
set.seed(12)
m <- 5500 #Number of random samples

MC <- numeric(0)
for (i in 1:1000){
  MC[i] <- mean(exp(runif(m)))
}   # simple Monte Carlo simulation

Anti <- numeric(0)
for (i in 1:1000){
  u <- runif(m/2) #half of the random samples
  v <- 1-u #another half of the random samples
  Anti[i] <- mean ((exp(u)+exp(v))/2)
}  # antithetic variate

v1 <- var(MC)
v2 <- var(Anti)
pr <- (v1-v2)/v1 #the percent reduction in variance.
pr
```

**Conclusion.** From the results, it is not difficult to see that the simulated variance decreases by 96.711% after using antithetic variables. This result is close to the theoretical value of 96.767%.

## 2022-10-01

## Question

1. Exercise 5.13 (Page 151, Statistical Computing with R). 

2. Exercise 5.15 (Page 151, Statistical Computing with R). 

## Answer

### Exercise 5.13

**Problem.**  Find two importance functions $\small f_1$ and $\small f_2$ that are supported on $\small (1,\infty)$  and are ‘close’ to $$\small g(x)=\displaystyle \frac{x^2}{\sqrt{2 \pi}}e^{-\frac{x^2}{2}}, \quad \quad x>1.$$

Which of your two importance functions should produce the smaller variance
in estimating $$\small \int_{1}^{\infty} \displaystyle \frac{x^2}{\sqrt{2 \pi}}e^{-\frac{x^2}{2}} \; dx$$
by importance sampling? Explain.

**Solution.** The following three steps will be used to solve the problem.

***Step 1: Select the appropriate function $\small f_1(x)$ and $\small f_2(x)$.*** 

The normal distribution and lognormal distribution are considered as the importance functions $\small f_1(x)$ and $\small f_2(x)$ respectively. Their expression is as follows
$$\small f_1(x) = \displaystyle\frac{1}{\sqrt{2\pi}}e^{-\frac{(x-1)^2}{2}},\quad f_2(x)=\displaystyle\frac{1}{\sqrt{2\pi}x}e^{-\frac{(lnx-1)^2}{2}}.$$
Compared to $\small f_ 2(x)$, $\small f_ 1(x)$ is closer to the multiple of $g (x)$, so $\small f_1(x)$ should produce the smaller variance.

***Step 2: Diagrams of importance function $\small f(x)$ and the ratio $\small g(x)/f(x)$.*** 

```{r,echo=FALSE,fig.width=10}
    x <- seq(1, 10, 0.01)
    w <- 2

    g <- ((x^2)*exp(-x^2/2)) / sqrt(2*pi)
    f1 <- 2* dnorm (x, 1)
    f2 <- 2* dlnorm(x, 1) 
    gs <- c(expression(g(x)),
            expression(f[1](x)),
            expression(f[2](x)))
    
    #for color change lty to col
    par(mfrow=c(1,2))
    
    #figure (a)
    plot(x, g, type = "l", ylab = "",
         ylim = c(0,1), lwd = w, col=1, main='(A):the distribution g(x) & f(x)')
    lines(x, f1, lty = 2, lwd = w, col=2)
    lines(x, f2, lty = 3, lwd = w, col=3)
    legend("topright", legend = gs,
           lty = 1:3, lwd = w, inset = 0.02,col=1:3)

    #figure (b)
    plot(x, g/f1, type = "l", ylab = "",
        ylim = c(0,1), lwd = w, lty = 2, col=2, main='(B):the ratio g(x)/f(x)')
    lines(x, g/f2, lty = 3, lwd = w,col=3)
    legend("topright", legend = gs[-1], 
           lty = 2:3, lwd = w, inset = 0.02, col=2:3)
```
From figure **(A)**, it is not difficult to see that the distribution of $\small f_1(x)$ and $\small f_2(x)$ are not very close to $\small g(x)$. But the shape of $\small f_1(x)$ is more similar to the shape of $\small g(x)$.

From figure **(B)**, it is not difficult to see the change of the ratio $\small g(x)/f_1(x)$ is more gentle, and its peak value does not exceed 0.5. However, the ratio $\small g(x)/f_2(x)$ changes sharply in the early stage, with the peak value exceeding 0.5. Thus, the ratio $\small g(x)/f_1(x)$ works better.

***Step 3: Calculate the variance of the ratio $\small g(x)/f(x)$.*** 

The code is shown as follows.
```{r}
  set.seed(22072)
  m <- 10000
  est <- sd <- numeric(2)
  g <- function(x) {
  ((x^2)*exp(-x^2/2)) / sqrt(2*pi)*(x>1)
  }
  
  x1 <- rnorm(m,1) #using f1-norm
  fg1 <- g(x1)/ dnorm(x1,1)
  est[1] <- mean(fg1)
  sd[1] <- sd(fg1)
  
  x2 <- rlnorm(m,1) #using f2-lnorm
  fg2 <- g(x2)/ dlnorm(x2,1)
  est[2] <- mean(fg2)
  sd[2] <- sd(fg2)
  
  res <- rbind(est=round(est,3), sd=round(sd,3)) 
  #the matrix containing mean and variance of the ratio g(x)/f(x)
  res
```

It is not difficult to see that the mean values of the two ratios are very close (0.401 and 0.404), but the former (0.404) is better than the latter (0.521) in terms of variance.

**Conclusion.** Through the theoretical analysis, diagrams and numerical simulation results, it is not difficult to see that the importance function $\small f_1(x)$ is better than $\small f_2(x)$.


### Exercise 5.15

**Problem.**  Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

**Solution.** The following two steps will be used to solve the problem.

***Step 1:  Use stratified sampling method to estimate the integral. *** 

For $$\small \int_{0}^{1} \displaystyle\frac{e^{-x}}{1+x^2} \; dx.$$ 

The importance function is as follows
$$\small f(x) = \displaystyle\frac{e^{-x}}{1-e^{-1}},\quad 0<x<1.$$
Now divide the interval (0,1) into five sub-intervals, $\small \bigg( \displaystyle\frac{j}{5},\displaystyle\frac{j+1}{5}\bigg),j=0,1,\cdots,4$. Then on the $\small j^{th}$sub-interval variables are generated from the density
$$\small f_j(x) = \displaystyle\frac{5e^{-x}}{1-e^{-1}},\quad \displaystyle\frac{j}{5}<x<\displaystyle\frac{j+1}{5}.$$
The code is shown as follows.

```{r}
set.seed(22072)
M <- 10000
k <- 5 
m <- M/k
N <- 50
si <- matrix(0, N, k)
v <- matrix(0, N, k)
gf <- function(x) (exp(-x)/(1 + x^2))/((k/(1 - exp(-1))) * exp(-x)) #g/f
for (i in 1:N) {
  for (j in 1:k) {
  #Generate random number by inverse function method
  x <- -log(1 - (1 - exp(-1)) * runif(m, (j - 1)/k, j/k)) 
  si[i,j] <- mean(gf(x))
  v[i,j] <- var(gf(x))
  }  
}

round(mean(rowSums(si)),4)       #\theta_hat^(S)
round(sqrt(mean(colMeans(v))),5) #standard deviation
```


***Step 2: compare step1 with the result of Example 5.10.***

Example 5.10 is a non-stratified importance sampling method and its code is shown as follows.
```{r}
set.seed(22072)
M <- 10000
k <- 1  #non-stratified 
m <- M/k
si <- numeric(k)
v <- numeric(k)
gf <- function(x) (exp(-x)/(1 + x^2))/((k/(1 - exp(-1))) * exp(-x)) #g/f
for (j in 1:k) {
  #Generate random number by inverse function method
  x <- -log(1 - (1 - exp(-1)) * runif(m, (j - 1)/k, j/k)) 
  si[j] <- mean(gf(x))
  v[j] <- var(gf(x))
}

round(sum(si),4)       #\theta_hat^(M)
round(sqrt(mean(v)),5) #standard deviation
```


**Conclusion.** It is not difficult to see that the integral values of importance sampling and stratified sampling simulation are very close, but stratified sampling (0.00418) can reduce more variance than importance sampling (0.09585).

## 2022-10-09

## Question

1. Exercise 6.4 (Page 180, Statistical Computing with R).

2. Exercise 6.8 (Page 181, Statistical Computing with R). 

3. Discussion (Page 26, Chap04-Monte Carlo Methods in Inference). 

## Answer

### Exercise 6.4

 **Problem.** Suppose that $\small X_1,\cdots,X_n$ is a random sample from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter $\small \mu$.Use a Monte Carlo method to obtain an empirical estimate of the confidence level.

**Solution.** The following two steps will be used to solve the problem.

***Step 1: Construct a 95% confidence interval for the parameter $\small \mu$.***

Firstly, the maximum likelihood estimators of $\small \mu$ and $\small \sigma^2$ are recorded respectively $\small \hat{\mu}$ and $\small \hat{\sigma}^2$. Their expressions are as follows
$$\small \hat{\mu}=\displaystyle\frac{1}{n} \sum_{i=1}^{n}lnX_i,\quad \hat{\sigma}^2=\displaystyle\frac{1}{n}\sum_{i=1}^{n}\Big(lnX_i-\displaystyle\frac{1}{n} \sum_{i=1}^{n}lnX_i\Big)^2.$$
Actually, $\small \hat{\mu}$ is an unbiased estimator and consistent estimator of
$\small \mu$. However, $\small \hat{\sigma}^2$ is a asymptotic unbiased estimator and consistent estimator of $\small \sigma^2$. $\small \hat{\sigma}^2$ can be modified to $\small \hat{\sigma_*}^2$ and then become an unbiased estimator of $\small \sigma^2$.

$$\small \hat{\sigma_*}^2 = \displaystyle\frac{1}{n-1}\sum_{i=1}^{n}\Big(lnX_i-\displaystyle\frac{1}{n} \sum_{i=1}^{n}lnX_i\Big)^2 = \displaystyle\frac{n}{n-1}\hat{\sigma}^2.$$
When $\small \sigma^2$ is unknown, the test statistics for $\small \mu$ is as follows

$$\small T=\displaystyle\frac{\displaystyle\frac{1}{n}\sum_{i=1}^{n}lnX_i-\mu}{\hat{\sigma_*}/\sqrt{n}} \sim t(n-1).$$ 
Then for a given  $\small (1-\alpha)$ confidence level，the following probability holds

$$\small P\left\{\small -t_{\alpha/2}(n-1) < \displaystyle\frac{\displaystyle\frac{1}{n}\sum_{i=1}^{n}lnX_i-\mu}{\hat{\sigma_*}/\sqrt{n}} < t_{\alpha/2}(n-1)\right\}=1-\alpha.$$
namely,
$$\small P\left\{\displaystyle\frac{1}{n}\sum_{i=1}^{n}lnX_i-\displaystyle\frac{\hat{\sigma_*}}{\sqrt{n}}t_{\alpha/2}(n-1)<\mu<\displaystyle\frac{1}{n}\sum_{i=1}^{n}lnX_i+\displaystyle\frac{\hat{\sigma_*}}{\sqrt{n}}t_{\alpha/2}(n-1)\right\}=1-\alpha.$$
Thus, when $\small (1-\alpha)=95\%$, the confidence interval for the parameter $\small \mu$ is
$$\small \left[\displaystyle\frac{1}{n}\sum_{i=1}^{n}lnX_i-\displaystyle\frac{\hat{\sigma_*}}{\sqrt{n}}t_{0.025}(n-1),\displaystyle\frac{1}{n}\sum_{i=1}^{n}lnX_i+\displaystyle\frac{\hat{\sigma_*}}{\sqrt{n}}t_{0.025}(n-1)\right].$$

***Step 2: Use a Monte Carlo method to obtain an empirical estimate of the confidence level.*** 

Actually, when the random variable $\small X$ obeys the lognormal distribution, $\small lnX$ obeys the normal distribution, which means $\small lnX \sim N(\mu,\sigma^2)$.Thus,$\small X$ can be transformed to a normal random variable $\small lnX$ and estimate $\small \mu$ with the sample mean of the transformed sample. The code is shown as follows.(Take 50 samples each time and repeat 10000 times in total.)

```{r}
#Clear all variables
rm(list = ls())

###(1).Data generation
set.seed(22072)
n <- 50               #sample size
m <- 10000            #number of Repeating sample generation
#initialization
CI <- matrix(0,2,m)
#Generate random number
RN <- function(n1, m1) {
  x <- y <- matrix(0,n1,m1)
  for (i in 1:m1){
    x[,i] <- rlnorm(n1)
  }
  y<- log(x)    #transform X
  return(y)
}                     

###(2).Data analysis
#Calculate sample mean and standard deviation
ybar_se <- function(y1){
  result <- list(ybar=apply(y1,2,mean),se= apply(y1,2,sd)/sqrt(n)) 
}
#Calculate sample confidence interval
CI[1,] <- ybar_se(RN(n,m))$ybar+ybar_se(RN(n,m))$se*qnorm(0.025)  #Lower bound of CI
CI[2,] <- ybar_se(RN(n,m))$ybar+ybar_se(RN(n,m))$se*qnorm(0.975)  #Upper bound of CI

###(3).result reporting
meann <- function(ci){
  mean(ci[1,] < 0 & ci[2,]> 0)*100
}
meann(CI)
```
**Conclusion.** it is not difficult to see that our empirical estimate of the confidence level is 95.2% which is very close to the given confidence level 95%.


[Back to the Question](#question)


### Exercise 6.8

**Problem.** Refer to Example 6.16. Repeat the simulation, but also compute the $\small F$ test of equal variance, at significance level $\small \hat{\alpha} \overset{.}{=} 0.055$. Compare the power of the Count Five test and $\small F$ test for small, medium and large sample sizes. (Recall that the $\small F$ test is not applicable for non-normal distributions.)

**Solution.** The following two steps will be used to solve the problem.

***Step 1:  Review the Assumptions in Example 6.16. *** 

In Example 6.16, the sample distributions are $\small N(\mu_1=0,\sigma^2=1)$ and $\small N(\mu_1=0,\sigma^2=1.5^2)$.


***Step 2:  Compare the power of the Count Five test and $\small F$ test for different sample sizes. *** 

In this question, the small, medium and large sample sizes will be chosen as 20,50 and 100,then we will sample 10000 times respectively. The code is shown as follows.
```{r}
#Clear all variables
rm(list = ls())

###(1).Data gereration
set.seed(22072)
sigma1 <- 1
sigma2 <- 1.5
n1 <- 20   #small sample size
n2 <- 50   #median sample size
n3 <- 100  #large sample size
m <- 10000 #number of Repeating sample generation

RN <- function(n_1, m_1) {
  x1 <- y1 <- matrix(0,n_1,m_1)
  for (i in 1:m_1){
    x1[,i] <- rnorm(n_1, 0, sigma1)
    y1[,i] <- rnorm(n_1, 0, sigma2)
  }
  result <- list(x=x1,y=y1)
  return(result)
}   
#RN(n1,m) small sample
#RN(n2,m) median sample
#RN(n3,m) large sample

###(2).Data analysis
# count5test 
count5test <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(as.integer(max(c(outx, outy)) > 5))  
}

# Ftest
Fp <- function(x, y){
  Fp <- var.test(x, y)$p.value       #p.value for Ftest
  return(as.integer(Fp <= 0.055))   
}

###(3).result reporting
tests <- function(x,y){
  c5 <- ftest <- numeric(0)
  for (i in 1:m){
    c5[i] <- count5test(x[,i],y[,i])
    ftest[i] <- Fp(x[,i],y[,i])
  }
  result <- list(C5p=round(mean(c5),4),Ftestp=round(mean(ftest),4))
  return(result)          
}

tests(RN(n1,m)$x,RN(n1,m)$y)  #power of small sample
tests(RN(n2,m)$x,RN(n2,m)$y)  #power of median sample
tests(RN(n3,m)$x,RN(n3,m)$y)  #power of large sample
```

**Conclusion.** The output results can be organized into the following table.

| Sample size | Count5test  | F-test      |
| :----:      | :----:      |    :----:   |          
| 20          | 0.3039      | 0.4045      | 
| 50          | 0.6568      | 0.8102      | 
| 100         | 0.8481      | 0.9819      | 

It is not difficult to see that for all different sample sizes, F-test is more powerful than Count5test.


### Discussion

**Problem.**

(1). If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

(2). What is the corresponding hypothesis test problem?

(3). Which test can we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why?

(4). Please provide the least necessary information for hypothesis testing.

**Answer.** 

***(1).*** We **cannot** say the powers are different at 0.05 level. We need to judge according to the results of hypothesis testing.

***(2).*** The corresponding hypothesis for the test problem is whether the powers are different, that is $$\small H_{0}: power_1=power_{2} \quad vs \quad H_{1}: power_{1} \neq power_{2}.$$

***(3).*** We should use **paired-t test**. Because two methods are both under a particular simulation setting with 10,000 experiments, that means they use the same sample. Thus,the difference between the powers corresponding to the two methods is approximately obeys the t distribution.

***(4).*** There is only one sample for each of $\small power_{1}$ and $\small power_{2}$, so 10,000 experiments should be performed to generate a series of $\small power_{1}$ and $\small power_{2}$, namely to obtain a series of differences between $\small power_{1}$ and $\small power_{2}$ which can be recorded as $\small d$. $\small d$ approximately obeys the $\small t$ distribution. The null hypothesis can be tested by applying paired-t test.

## 2022-10-14

## Question

1. Exercise 7.4 (Page 212, Statistical Computing with R).

2. Exercise 7.5 (Page 212, Statistical Computing with R). 

3. Exercise 7.A (Page 213, Statistical Computing with R). 

## Answer

### Exercise 7.4
 **Problem.** Refer to the air-conditioning data set **aircondit** provided in the **boot** package. The 12 observations are the times in hours between failures of airconditioning equipment.
$$\small 3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487.$$
Assume that the times between failures follow an exponential model $\small Exp(\lambda).$ Obtain the MLE of the hazard rate $\small \lambda$ and use bootstrap to estimate the bias and standard error of the estimate.

**Solution.** The following two steps will be used to solve the problem.

***Step 1: Solve the maximum likelihood estimator of the parameter $\small \lambda$.*** 

Let $\small X_1,..., X_n$ be the samples obeyed the exponential distribution population $\small Exp(\lambda)$ and the probability density function is
$$\small f(x)=\lambda e^{-\lambda x} \cdot 1\{x\geq 0\}, \quad \lambda>0.$$
So the maximum likelihood function is
$$\small L(x;\lambda)=\prod_{i=1}^n \lambda e^{-\lambda x_i}=\lambda^n \cdot exp\{-\lambda \sum_{i=1}^n x_i\}$$
and the logarithmic likelihood is 
$$\small lnL(x;\lambda)=nln\lambda-\lambda\sum_{i=1}^n x_i.$$
Then let the first derivative of the logarithmic likelihood function equal to zero
$$\small \displaystyle\frac{dlnL(x;\lambda)}{d\lambda}=\displaystyle\frac{n}{\lambda}-\sum_{i=1}^n x_i=0.$$
The above equation can be solved and the maximum likelihood estimator of the parameter $\small \lambda$ is
$$\small \hat{\lambda}=\displaystyle\frac{n}{\sum_{i=1}^n x_i}=\displaystyle\frac{1}{\overline{x}}.$$
Then the MLE of the hazard rate $\small \lambda$ is $\small \displaystyle\frac{1}{\overline{x}}.$

***Step 2: Use bootstrap to estimate the bias and standard error of the estimate.***

The code is shown as follows.

```{r}
    #Clear all variables
    rm(list = ls())
    
    ###(1).Data generation
    library(boot)               #install R Package
    set.seed(22072)             
    i1 <- 1
    yangben <- function(i){
      x <- as.matrix(aircondit[i])
      return(x)
    }
    #yangben(i1)                #obtain samples as matrix form
    
    ###(2).Data analysis
    lambda <- function(x,i) {
      return(1/mean(x[i,]))
    }                           #MLE of lambda 
    
    ###(3).result reporting
    result <- function(i){
      boot(yangben(i), statistic = lambda, R = 3000)
    }
    result(i1)

    detach(package:boot)        #Uninstall R Package
```
**Conclusion.** the bias and standard error of the estimate are 0.001290248 and 0.00432681 respectively which shows that the effect of bootstrap method is good.

### Exercise 7.5

**Problem.** Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $\small 1/\lambda$ by the standard normal, basic, percentile and BCa methods. Compare the intervals and explain why they may differ.

**Solution.** The following two steps will be used to solve the problem.

***Step 1:  Review bootstrap confidence intervals for the parameter $\small \theta$ in those four methods.*** 

(1).Standard normal method:
$$\small (\hat\theta-z_{1-\alpha/2}\hat{se}(\hat\theta) ,\ \hat\theta-z_{\alpha/2}\hat{se}(\hat\theta)).$$
(2). Basic method:
$$\small (2\hat\theta-\hat\theta^*_{1-\alpha/2} ,\ 2\hat\theta-\hat\theta^*_{\alpha/2})$$
where $\small \hat\theta^*_a$ is the $\small a$-quantile of $\small \hat\theta^*$.

(3). Percentile method:
$$\small (\hat\theta^*_{\alpha/2} ,\ \hat\theta^*_{1-\alpha/2}).$$
(4). BCa method:
$$\small (\hat\theta^*_{\alpha_1} ,\ \hat\theta^*_{\alpha_2})$$
where $$\small \alpha_1=\Phi\left(\hat z_0+\frac{\hat z_0+z_{\alpha/2}}{1-\hat a (\hat z_0+z_{\alpha/2})}\right), \alpha_2=\Phi\left(\hat z_0+\frac{\hat z_0+z_{1-\alpha/2}}{1-\hat a (\hat z_0+z_{1-\alpha/2})}\right),$$

$$\small \hat z_0=\Phi^{-1}\left(\frac1B\sum_{b=1}^BI(\hat\theta^{(b)}<\hat\theta)\right), \hat a=\frac{\sum_{i=1}^n(\bar{\theta}_{(\cdot)}-\theta_{i})^3} {6\sum_{i=1}^n((\bar{\theta}_{(\cdot)}-\theta_{i})^2)^{3/2}}.$$
***Step 2: Compute 95% bootstrap confidence intervals for the mean time between failures $\small 1/\lambda$ by the standard normal, basic, percentile and BCa methods.*** 

The code is shown as follows.
```{r}
    #Clear all variables
    rm(list = ls())
    
    ###(1).Data generation
    library(boot)                #install R Package
    library(ggplot2)
    set.seed(22072)    
    
    i1 <- 1
    yangben <- function(i){
      x <- as.matrix(aircondit[i])
      return(x)
    }
    #yangben(i1)                 #obtain samples as matrix form
    
    ###(2).Data analysis
    meantime <- function(x,i) {
      return(mean(x[i,]))
    }                            #return 1/lambda 
    
    estimat <- function(i){
      boot(yangben(i), statistic = meantime, R = 3000)
    }
    estimat(i1)
    
    ###(3).result reporting
    result <- function(i){
      CI <- boot.ci(estimat(i), type = c("norm", "basic", "perc", "bca"))
      cat('The CI in four methods:','\n','normal CI:', CI$norm[2:3], '\n','basic CI:',
      CI$basic[4:5],'\n','percentile CI:', CI$percent[4:5],'\n','BCa CI:',
      CI$bca[4:5])}
    result(i1)
    
    #histogram 
    ggplot(data.frame(estimat(i1)$t),aes(estimat(i1)$t))+geom_histogram(binwidth = 10)+labs(x='estimate', y=' Frequency')+ggtitle("Histogram of estimate")
    
    detach(package:boot)       #Uninstall R Package
```

**Conclusion.** The output results can be organized into the following table.

| Method          | Lower bound of CI  | Upper bound of CI |
| :----:          | :----:             |    :----:         |          
| standard normal | 35.53352           |  183.0605         | 
| basic           | 25.26257           |  170.9979         | 
| percentile      | 45.16876           |  190.9041         | 
| BCa             | 57.77506           |  228.3228         | 

The conclusions about differences between CIs and reasons about these results are as follow.

(1).The replicates are not approximately normal, so the standard normal and percentile intervals are differ.

(2).From the histogram of estimates, it is not difficult to see that the distribution of the estimates is skewed. However, the BCa interval can adjust both skewness and bias in distribution which has better application here. 

(3). In addition, the sample size is too small for the central limit theorem to give a good approximation.

### Exercise 7.A

**Problem.** Conduct a Monte Carlo study to estimate the coverage probabilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence
interval and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample mean. Find the proportion of times that the confidence intervals miss on the left and the proportion of times that the confidence intervals miss on the right.

**Solution.** The specified true normal distribution is $\small N(0,2.88^2)$and the sample size for each time is 30. Then the code is shown as follows.
```{r}
   #Clear all variables
    rm(list = ls())

   ###(1).Data generation
   library(boot)        #install R Package
   set.seed(22072)
   
   mu <- 0              #true mean
   sigma <-2.88
   n <- 30              #sample size
   m <- 2000            #replicate
   yangben <- function(sn,s,sig){
     return(rnorm(sn,s,sig))
   }                    #Sampling function

   ###(2).Data analysis
   avg <- function(x,i){
     return(mean(x[i]))
   }
  
   CI.norm <- CI.basic <- CI.perc <- matrix(0, m, 2)
   for (j in 1:m) {
     avg1 <- boot(yangben(n,mu,sigma), statistic = avg, R=2000)
     CI <- boot.ci(avg1, type=c("norm","basic","perc"))
     CI.norm[j,] <- CI$norm[2:3]
     CI.basic[j,] <- CI$basic[4:5]
     CI.perc[j,] <- CI$percent[4:5]
   }
   
   ###(3).result reporting
   result <- function(s1){
   #the empirical coverage rates for the sample mean
    norm.cp <- mean(CI.norm[,1] <= s1 & CI.norm[,2] >= s1)
    basic.cp <- mean(CI.basic[,1] <= s1 & CI.basic[,2] >= s1)
    perc.cp <- mean(CI.perc[,1] <= s1 & CI.perc[,2] >= s1)
   #the proportion of times that the confidence intervals miss on the left
    norm.leftmiss <- mean(CI.norm[,1] >= s1)
    basic.leftmiss <- mean(CI.basic[,1] >= s1)
    perc.leftmiss <- mean(CI.perc[,1] >=s1)
   #the proportion of times that the confidence intervals miss on the right
    norm.rightmiss <- mean(CI.norm[,2] <= s1)
    basic.rightmiss <- mean(CI.basic[,2] <= s1)
    perc.rightmiss <- mean(CI.perc[,2] <= s1) 
    #Result modification
    cp <- c(norm.cp,basic.cp,perc.cp)
    Left.miss <- c(norm.leftmiss,basic.leftmiss,perc.leftmiss)
    Right.miss <- c(norm.rightmiss,basic.rightmiss,perc.rightmiss)
    df <- data.frame(cp,Left.miss,Right.miss)
    row.names(df) <- c("standard normal","basic","percentile")
    return(df)
   }
   result(mu)
   
   detach(package:boot)       #Uninstall R Package
```

**Conclusion.** It is not difficult to see that when the sample size is 20, the coverage probabilities of these 3 types of confidence intervals are very **close to 0.95**. In addition, the least proportion of times that the confidence intervals miss on the left is 0.0320 (basic method) and the highest proportion of times that the confidence intervals miss on the left is 0.0345	(percentile method). Similarly, the least proportion of times that the confidence intervals miss on the right is 0.0225  (standard normal & percentile method), while the highest proportion of times that the confidence intervals miss on the right is 0.0230 (basic method). In general, there is little difference between the highest one and the lowest one.

## 2022-10-21

## Question

1. Exercise 7.8 (Page 213, Statistical Computing with R). 

2. Exercise 7.11 (Page 213, Statistical Computing with R). 

3. Exercise 8.2 (Page 242, Statistical Computing with R).

## Answer

### Exercise 7.8
 **Problem.** Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\small \hat{\theta}.$
 
**Solution.** The following three steps will be used to solve the problem.

***Step 1: Review the jackknife estimate of bias and standard error. *** 

**(1). The jackknife estimate of bias.**

$$\small \widehat{bias}_{jack} = (n-1)(\overline{\hat{\theta}_{(\cdot)}}-\hat{\theta}),$$
where $\small \overline{\hat{\theta}_{(\cdot)}}=\displaystyle\frac{1}{n}\sum_{i=1}^{n}\hat{\theta}_{(i)}$ is the mean of the estimates from the leave-one-out
samples and $\small \hat{\theta}=\hat{\theta}(x)$ is the estimate computed from the original observed sample.

**(2). The jackknife estimate of standard error.**

A jackknife estimate of standard error is

$$\small \hat{se}_{jack}=\sqrt{\displaystyle\frac{n-1}{n}\sum_{i=1}^{n}\left(\hat{\theta}_{(i)}-\overline{\hat{\theta}_{(i)}}\right)^2},$$
for smooth statistics $\small \hat{\theta}$.

***Step 2: Review the parameter $\small \theta$ and its statistic $\small \hat{\theta}$ in exercise 7.7.***

The five-dimensional scores data **scor (bootstrap) ** have a 5 × 5 covariance matrix $\small \Sigma$, with positive eigenvalues $\small \lambda_1 > \cdots > \lambda_5$. In principal components analysis, the parameter $\small \theta$ has the following form
$$\small \theta = \displaystyle\frac{\lambda_1}{\sum_{i=1}^{5}\lambda_j}.$$
measures the proportion of variance explained by the first principal component. Let $\small \hat{\lambda}_1 > \cdots > \hat{\lambda}_5$ be the eigenvalues of $\small \hat{\Sigma}$, where $\small \hat{\Sigma}$ is the MLE of $\small \Sigma$. The sample estimate of $\small \theta$ is 

$$\small \hat{\theta}= \displaystyle\frac{\hat{\lambda}_1}{\sum_{i=1}^{5}\hat{\lambda}_j}.$$
***Step 3: Obtain the jackknife estimates of bias and standard error of $\small \hat{\theta}.$ ***

The code is shown as follows.

```{r}
    #Clear all variables
    rm(list = ls()) 
    
    ###(1).Data generation
    library(bootstrap)        #install R Package
    attach(scor)
    x <- function(i){
      as.matrix(i)
    }
    #Call example: x(scor)
    
    ###(2).Data analysis
    n <- nrow(x(scor))
    theta.hat <- function(j){
      lambda <- eigen(cov(j))$values
      theta_hat <- max(lambda/sum(lambda))
      return(round(theta_hat,8))
    }
    #Call example: theta.hat(x(scor))
    
    theta_jack <- numeric(0)
    theta.jack <- function(x){
      for (i in 1:n){
        y1 <- x[-i, ]
        s1 <- cov(y1)
        lambda1 <- eigen(s1)$values
        theta_jack[i] <- max(lambda1/sum(lambda1))
      }
      return(theta_jack)
    }
    #Call example: theta.jack(x(scor))
    
    bias_jack <- function(d1,d2){
      round((n - 1)*(mean(d1) - d2),8)
    }
    #Call example: bias_jack(theta.jack(x(scor)),theta.hat(x(scor)))
    
    se_jack <- function(d3){
      round(sqrt((n - 1)/n * sum((d3 - mean(d3))^2)),8)
    }
    #Call example: se_jack(theta.jack(x(scor)))
    
    ###(3).result reporting
    result <- function(d4,d5,d6){
      df <- data.frame(theta.hat=d4,jack.bias=d5,jack.se=d6)
      return(df)
    }
    result(theta.hat(x(scor)), bias_jack(theta.jack(x(scor)),theta.hat(x(scor))), se_jack(theta.jack(x(scor))))
    
    detach(scor) 
    detach(package:bootstrap)    #Uninstall R Package
```
**Conclusion**. The jackknife estimates of bias and standard error are about 0.001 and 0.05 respectively. They are not very different from the bootstrap estimates.

### Exercise 7.11

**Problem.** In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

**Solution.** The following three steps will be used to solve the problem.

***Step 1: Review information about the data and alternative models.*** 

The **ironslag (DAAG)** data has 53 measurements of iron content by two methods, **chemical and magnetic**. We hope to use mathematical models to describe the relationship between these two methods. The proposed models for predicting magnetic measurement (Y) from chemical measurement (X) are as follows.

(1). **Linear**: $\small Y=\beta_0 +\beta_1 X+ \varepsilon.$

(2). **Quadratic**: $\small Y=\beta_0 +\beta_1 X+\beta_2 X^2 + \varepsilon.$

(3). **Exponential**: $\small log(Y)=log(\beta_0) +\beta_1 X + \varepsilon.$

(4). **Log-Log**: $\small log(Y)=\beta_0 +\beta_1 log(X) + \varepsilon.$

(5). **Cubic polynomial(Ex.7.17)**: $\small Y=\beta_0 +\beta_1 X+\beta_2 X^2 + \beta_3 X^3 + \varepsilon.$

***Step 2: Procedure to estimate prediction error by leave-two-out cross validation.*** 

(1). For $\small k_1, k_2 \in \{1,...,n\}(k_1 < k_2)$, let observation $\small (x_{k_1}, y_{k_1}),(x_{k_2},y_{k_2})$ be the test points and use the remaining observations to fit the model.

(2). Estimate the sum of the squared prediction errors $\small \hat{\sigma}_\epsilon^2 =\displaystyle\frac{1}{C_n^2}\sum_{k_1=1}^{n-1}\sum_{k_{2}=k_{1}+1}^{n}(e_{k_{1}}^2+e_{k_{2}}^2),$ where $\small e_{k_i}=y_{k_i}-\hat{y_{k_i}},i=1,2.$ 

***Step 3: Use leave-two-out cross validation to compare the models.*** 

The code is shown as follows.

```{r}
    #Clear all variables
    rm(list = ls())
    
    ###(1).Data generation
    library(DAAG, warn.conflict = FALSE) 
    attach(ironslag)
    x_1 <- function(i){
      as.matrix(i)
    }
    #Call example: x_1(ironslag)
    
    ###(2).Data analysis
    n <- length(magnetic) 
    N <- choose(n, 2)    #Combination number
    e_1 <- e_2 <- e_3 <- e_4 <- e_5 <- numeric(N) 
    l <- 1 
    for (k1 in 1:(n - 1)){
      for (k2 in (k1 + 1):n){
        k <- c(k1, k2)
        y <- magnetic[-k]
        x <- chemical[-k]
  
        #Linear
        model_1 <- lm(y ~ x)
        yhat_1 <- function(model){
          model$coef[1] + model$coef[2] * chemical[k]
        }
        e_1[l] <- sum((magnetic[k] - yhat_1(model_1))^2)
        
        #Quadratic
        x2 <- x^2
        model_2 <- lm(y ~ x + x2)
        yhat_2 <- function(model){
          model$coef[1] + model$coef[2] * chemical[k] +model$coef[3] * chemical[k]^2
        }
        e_2[l] <- sum((magnetic[k] - yhat_2(model_2))^2)
  
        #Exponential
        model_3 <- lm(log(y) ~ x)
        
        yhat_3 <- function(model){
          log_yhat_3 <- model$coef[1] + model$coef[2] * chemical[k]
          return(exp(log_yhat_3))
        }
        e_3[l] <- sum((magnetic[k] - yhat_3(model_3))^2)
  
        #Log-Log
        model_4 <- lm(log(y) ~ log(x))
        yhat_4 <- function(model){
          log_yhat_4 <- model$coef[1] + model$coef[2] * log(chemical[k])
          return(exp(log_yhat_4))
        }
        e_4[l] <- sum((magnetic[k] - yhat_4(model_4))^2)
        
        #Cubic-polynomial(Ex.7.17)
        x3 <- x^3
        model_5 <- lm(y ~ x + x2 + x3)
        yhat_5 <- function(model){
          model$coef[1] + model$coef[2] * chemical[k] +model$coef[3] * chemical[k]^2 + model$coef[4] * chemical[k]^3
        }
        e_5[l] <- sum((magnetic[k] - yhat_5(model_5))^2)
        
        #iteration
        l <- l + 1
        }
      }
    
    ###(3).result reporting
    result <- function(a,b,c,d,e){
      df <- data.frame(linear=a,Quadratic=b,Exponential=c,Log_Log=d,Cubic_polynomial=e)
      return(df)
    }
    result(mean(e_1),mean(e_2),mean(e_3),mean(e_4),mean(e_5))
    
    detach(ironslag)
    detach(package:DAAG)   #Uninstall R Package
```

**Conclusion.** According to the minimum prediction error, it is not difficult to know that the quadratic model is the best one which has the minimum  prediction error(35.74037).

### Exercise 8.2

**Problem.** Implement the bivariate Spearman rank correlation test for independence as a permutation test. The Spearman rank correlation test statistic can be obtained from function **cor** with **method = "spearman"**. Compare the achieved significance level of the permutation test with the p-value reported by **cor.test** on the same samples.

**Solution.** Two samples with different distributions can be used to compare the p-values before and after the permutation: (1) multivariate normal; (2) Logarithmic multivariate normal, their means and covariance matrices are shown respectively as variable **mu_ 1** and **sigma_ 1** in the following code.
```{r}
   #Clear all variables
   rm(list = ls())

   ###(1).Data generation
   library(MASS)
   set.seed(9847)
   mu_1 <- c(0, 0)
   sigma_1 <- matrix(c(1, 0.5, 0.5, 1), 2, 2)
   n_1 <- 30
   R_1 <- 499
   
   # First sample
   data_1 <- function(n,mu,sigma){
     mvrnorm(n, mu, sigma)
   }
   data1 <- data_1(n_1,mu_1,sigma_1)
  
   # Second sample
   data_2 <- function(n,mu,sigma){
     exp(mvrnorm(n, mu, sigma))
   }
   data2 <- data_2(n_1,mu_1,sigma_1)

   ###(2).Data analysis
   spear_f <- function(a,b){
     cor.test(a, b, method = "spearman")
   }
   spear_1 <- spear_f(data1[,1],data1[,2])
   spear_2 <- spear_f(data2[,1],data2[,2])
   
   rs <- numeric(R_1)
   spear.perm_f <- function(a,b,n,R) {
     test <- cor.test(a, b, method = "spearman")$estimate
     n <- length(a)
     for (i in 1:R){
       j <- sample(1:n)    #permutation
       rs[i] <- cor.test(a, b[j], method = "spearman")$estimate
     }
     rs1 <- c(test, rs)
     p_value <- mean(as.integer(test <= rs1))
     return(p_value)
   }
   spear.perm_1 <- spear.perm_f(data1[,1],data1[,2],n_1,R_1)
   spear.perm_2 <- spear.perm_f(data2[,1],data2[,2],n_1,R_1)
   
   ###(3).result reporting
   result <- function(a,b,c){
     df <- data.frame(spear_pval= a, spear_perm_pval= b, yangben=c)
     return(df)
   }
   result(spear_1$p.value, spear.perm_1,"bivariate normal")
   result(spear_2$p.value, spear.perm_2,"bivariate lognormal")
   
   detach(package:MASS)   #Uninstall R Package
```

**Conclusion.** It is not difficult to see that for different samples, the p-values in both tests (before and after the permutation) are close in value ((1). **bivariate normal**: 0.01292807(before) & 0.012(after); (2). **bivariate lognormal**: 0.00327702(before) & 0.002(after)) and both significant (less than 0.05). That means the first and second columns of the samples are not independent which is consistent with our initial assumption. 

## 2022-10-28

## Question

1. Exercise 9.4 (Page 277, Statistical Computing with R). 

2. Exercise 9.7 (Page 278, Statistical Computing with R). 

(Requirement:For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\small \hat{R} < 1.2.$)

## Answer

### Exercise 9.4
 **Problem.**  Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.
 
**Solution.** The following two steps will be used to solve the problem.

***Step 1: theoretical analysis.*** 

The standard Laplace density is $$\small f(x)= \displaystyle\frac{1}{2}e^{-|x|},$$ which is proportional to $\small e^{-|x|}$. Thus,$$\small \alpha(x_t,y)=min\left\{1,\displaystyle\frac{f(y)}{f(x_t)}\right\}=min\left\{1,\displaystyle\frac{e^{-|y|}}{e^{-|x_t|}}\right\}=min\left\{1,e^{|x_t|-|y|}\right\}.$$


***Step 2: Chain generation based on random walk Metropolis algorithm.***

In the following chain generator $\small rw.Metropolis\_Laplace$, we choose to compare the chains generated when the standard deviation $\small \sigma= 0.5, 1.5, 2, 2.5$. Corresponding to the same $\small \sigma$, we choose to compare the chains generated by different initial values $\small x_0=-5,5,10,20.$ 

**Firstly**, we use the **Gelman-Rubin** method to monitor the convergence of the chain generated by four different initial values under the same $\small \sigma$. The code can be shown as follows.
```{r}
rm(list=ls())                 #clear variables

###(1).Data generation
N <-6000                      #length of chains
sigma <- c(0.5, 1.5, 2, 2.5)  #parameter of different proposal distributions
x_0 <- c(-5, 5, 10, 20)       #choose over dispersed initial values
k1 <- length(x_0)             #number of chains generated by the same sigma corresponding to different initial values
b <- 500                      #burn-in length

rw.Metropolis_Laplace <- function(sigma, x0, N) {
  # sigma:  standard variance of proposal distribution N(x_t,sigma)
  # x_0: initial value
  # N: size of random numbers required.
  # k: Number of rejections
  set.seed(22072)
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, mean=x[i-1], sd=sigma)
    if (u[i] <= exp(abs(x[i-1])- abs(y)))
      x[i] <- y  
    else {
      x[i] <- x[i-1]
      k <- k + 1
    }
  }
  return(list(x=x, k=k))
}

###(2).Data analysis
Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi)     #row means
  B <- n * var(psi.means)        #between variance est.
  psi.w <- apply(psi, 1, "var")  #within variances
  W <- mean(psi.w)               #within est.
  v.hat <- W*(n-1)/n + (B/n)     #upper variance est.
  r.hat <- v.hat / W             #G-R statistic
  return(r.hat)
}

#generate the chains
X1 <- X2 <- X3 <- X4 <- matrix(0, nrow=k1, ncol=N)
for (i in 1:k1)
    X1[i, ] <- rw.Metropolis_Laplace(sigma[1], x_0[i], N)$x
for (i in 1:k1)
    X2[i, ] <- rw.Metropolis_Laplace(sigma[2], x_0[i], N)$x
for (i in 1:k1)
    X3[i, ] <- rw.Metropolis_Laplace(sigma[3], x_0[i], N)$x
for (i in 1:k1)
    X4[i, ] <- rw.Metropolis_Laplace(sigma[4], x_0[i], N)$x

#compute diagnostic statistics
psi1 <- t(apply(X1, 1, cumsum))
for (i in 1:nrow(psi1))
  psi1[i,] <- psi1[i,] / (1:ncol(psi1))

psi2 <- t(apply(X2, 1, cumsum))
for (i in 1:nrow(psi2))
  psi2[i,] <- psi2[i,] / (1:ncol(psi2))

psi3 <- t(apply(X3, 1, cumsum))
for (i in 1:nrow(psi3))
  psi3[i,] <- psi3[i,] / (1:ncol(psi3))

psi4 <- t(apply(X4, 1, cumsum))
for (i in 1:nrow(psi4))
  psi4[i,] <- psi4[i,] / (1:ncol(psi4))
```

```{r}
###(3).Result reporting
#plot the sequence of R-hat statistics
rhat1 <- rep(0, N-b)
for (j in (b+1):N)
  rhat1[j-b] <- Gelman.Rubin(psi1[,1:j])
plot(rhat1[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

rhat2 <- rep(0, N-b)
for (j in (b+1):N)
  rhat2[j-b] <- Gelman.Rubin(psi2[,1:j])
plot(rhat2[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

rhat3 <- rep(0, N-b)
for (j in (b+1):N)
  rhat3[j-b] <- Gelman.Rubin(psi3[,1:j])
plot(rhat3[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

rhat4 <- rep(0, N-b)
for (j in (b+1):N)
  rhat4[j-b] <- Gelman.Rubin(psi4[,1:j])
plot(rhat4[(b+1):N], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)

#percent of candidate points accepted
X11 <- X21 <- X31 <- X41 <- matrix(0, nrow=k1, ncol=1)
for (i in 1:k1)
  X11[i] <- (N-rw.Metropolis_Laplace(sigma[1], x_0[i], N)$k)/N
for (i in 1:k1)
  X21[i] <- (N-rw.Metropolis_Laplace(sigma[2], x_0[i], N)$k)/N
for (i in 1:k1)
  X31[i] <- (N-rw.Metropolis_Laplace(sigma[3], x_0[i], N)$k)/N
for (i in 1:k1)
  X41[i] <- (N-rw.Metropolis_Laplace(sigma[4], x_0[i], N)$k)/N

result <- function(d4,d5,d6,d7){
  df <- cbind(X11,X21,X31,X41)
  colnames(df) <- c("sigma=0.5","sigma=1.5","sigma=2","sigma=2.5")
  row.names(df) <- c("IV(initial value)=-5", "IV=5", "IV=10","IV=20")
  return(df)
}
result(X11,X21,X31,X41)         #acceptance rates matrix
```

**Secondly**, we compare four chains generated by the proposal distribution corresponding to different $\small \sigma$ when the initial value $\small x_0=10$. The code can be shown as follows.
```{r}
#Comparison for acceptance rate
print(c(X11[3],X21[3],X31[3],X41[3]))

#Comparison for trace plot
plot(X1[3,], type = "l",xlab=bquote(sigma == 0.5),ylab="X")
plot(X2[3,], type = "l",xlab=bquote(sigma == 1.5),ylab="X")
plot(X3[3,], type = "l",xlab=bquote(sigma == 4.0),ylab="X")
plot(X4[3,], type = "l",xlab=bquote(sigma == 8.0),ylab="X")

#Comparison for histogram(Remove the sample in the pre burning period)
p <- ppoints(250)
y <- qexp(p, 1)
z1 <- c(-rev(y), y)
fx1 <- 0.5 * exp(-abs(z1))
hist(X1[3,][(b+1):N], breaks = "Scott", freq = FALSE, ylim = c(0,0.6))
lines(z1, fx1)
hist(X2[3,][(b+1):N], breaks = "Scott", freq = FALSE, ylim = c(0,0.6))
lines(z1, fx1)
hist(X3[3,][(b+1):N], breaks = "Scott", freq = FALSE, ylim = c(0,0.6))
lines(z1, fx1)
hist(X4[3,][(b+1):N], breaks = "Scott", freq = FALSE, ylim = c(0,0.6))
lines(z1, fx1)

#Comparison for QQ-plot(Remove the sample in the pre burning period)
Q1 <- quantile(X1[3,][(b + 1):N], p)
qqplot(z1, Q1, cex = 0.5)
abline(0, 1)
Q2 <- quantile(X2[3,][(b + 1):N], p)
qqplot(z1, Q2, cex = 0.4)
abline(0, 1)
Q3 <- quantile(X3[3,][(b + 1):N], p)
qqplot(z1, Q3, cex = 0.4)
abline(0, 1)
Q4 <- quantile(X4[3,][(b + 1):N], p)
qqplot(z1, Q4, cex = 0.4)
abline(0, 1)

#Comparison for ten quantiles
#install.packages("VGAM",repos ="http://cran.us.r-project.org")
library(VGAM)
a<- seq(0.05,0.95,0.1)
Q<- qlaplace(a, location = 0, scale = 1)    #theoretical ten quantiles
mc<-cbind(X1[3,][(b + 1):N],X2[3,][(b + 1):N],X3[3,][(b + 1):N],X4[3,][(b + 1):N])        
colnames(mc) <- c("sigma=0.5","sigma=1.5","sigma=2","sigma=2.5")
Qrw<- apply(mc,2,function(x) quantile(x,a)) #ten quantiles of chain
print(round(cbind(Q, Qrw), 3))        
detach(package:VGAM)
```

**Conclusion.** It is not difficult to draw the following conclusions:

(1). For the **statistic $\small \hat{R}$ used to monitoring convergence of chain**: When $\small \sigma=0.5$, the chain nearly converges and the convergence speed is very slow. When $\small \sigma=1.5$ and the iteration numbers of the chain is close to 1000, $\small \hat{R} < 1.2$ appears. And the convergence speed is relatively fast. When $\small \sigma=2,2.5$ and the iteration numbers is less than 1000, the chain converges quickly. Thus, we conclude that when the variance of the proposal distribution is small relative to the variance of the target distribution (It is **2** in this question), the generated chain usually converges slowly.

(2). For the **initial values**: Through the result generated by the function **result**, it is found that the initial value has little effect on the acceptance rate.

(3). For the **sigma**: Through the trace plots, each chain seems to have converged to the target Laplace distribution. The histograms and Q-Q plots drawn after discarding the first 500 samples show that when $\small \sigma=1.5,2,2.5$, the corresponding chains have the excellent fit. The comparison between the theoretical quantiles of the target Laplace distribution and the sample quantiles of these four chains also confirms the above conclusion.

### Exercise 9.7

**Problem.** Implement a Gibbs sampler to generate a bivariate normal chain $\small (X_t,Y_t)$ with zero means, unit standard deviations and correlation $\small 0.9.$ Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $\small Y=\beta_0+\beta_1X$ to the sample and check the residuals of the model for normality and constant variance.

**Solution.** The following four steps will be used to solve the problem.

***Step 1: Using Gibbs sampling to generate random numbers with bivariate normal distribution $\small N(0,0,1,1,0.9)$.***

In the following chain generator $\small gibbs\_bivariate$, we choose to compare the chains generated by three different paired initial values $\small dij(i=1,2,j=1,2,3).$ The code can be  shown as follows.
```{r}
rm(list=ls())                 #clear variables
set.seed(1234)
###(1).Data generation
#initialize constants and parameters
N <- 5000                     #length of chains
burn <- 1000                  #burn-in length
X <- matrix(0, N, 2)          #the chain, a bivariate sample    
rho <- 0.9                    #parameter of initial distribution
sigma1 <- sigma2 <- 1
mu1 <- mu2 <- 0
s1 <- sqrt(1-rho^2) * sigma1
s2 <- sqrt(1-rho^2) * sigma2
d11 <- 0.2; d21 <- 1.2        #initialize1 for chain1
d12 <- 0.7; d22 <- 2          #initialize2 for chain2
d13 <- 1.4; d23 <- 4          #initialize3 for chain3

#generate the chain
gibbs_bivariate <- function(n1,d1,d2){
  X[1, ] <- c(d1, d2)         #initialize
  for (i in 2:n1) {
  x2 <- X[i-1, 2]
  m1 <- mu1 + rho * (x2-mu2) * sigma1/sigma2
  X[i, 1] <- rnorm(1, m1, s1)
  x1 <- X[i, 1]
  m2 <- mu2 + rho * (x1-mu1) * sigma2/sigma1
  X[i, 2] <- rnorm(1, m2, s2)
  }
  b <- burn + 1
  x <- X[b:n1, ]   #samples after discarding the burn-in samples
  return(list(X=X, x=x))
}
```

***Step 2: Plot the generated sample after discarding a suitable burn-in sample.*** 
```{r}
###(2).Data analysis
X11 <- Y11 <- numeric(0)
X11 <- gibbs_bivariate(N,d11,d21)$x[, 1]  #X sample after remove the burn-in sample
Y11 <- gibbs_bivariate(N,d11,d21)$x[, 2]  #Y sample after remove the burn-in sample
picture <- function(x,y){
  plot(x, y, cex = 0.25,xlab=bquote(x),ylab=bquote(y))
  abline(h = 0, v = 0)
}
picture(X11,Y11)  
cor(gibbs_bivariate(N,d11,d21)$x)  #constant variance

Gelman.Rubin <- function(psi) {
  # psi[i,j] is the statistic psi(X[i,1:j])
  # for chain in i-th row of X
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi)       #row means
  B <- n * var(psi.means)          #between variance est.
  psi.w <- apply(psi, 1, "var")    #within variances
  W <- mean(psi.w)                 #within est.
  v.hat <- W*(n-1)/n + (B/n)       #upper variance est.
  r.hat <- v.hat / W               #G-R statistic
  return(r.hat)
}
```

***Step 3:*** Fit a simple linear regression model $\small Y=\beta_0+\beta_1X$ to the sample and check the residuals of the model for normality and constant variance.
```{r}
###(3).Result reporting
L <- lm(Y11 ~ X11)
summary(L)
#par(mfrow = c(1, 2))
qqnorm(L$res, cex = 0.25)
qqline(L$res)             #Check whether the residual error obeys the normal distribution
plot(L$fit, L$res, cex = 0.4)
abline(h = 0)             #Check the homogeneity of residual error
#par(mfrow = c(1, 1))

#Convergence of chain
#Comparison for trace plot
X21 <- Y21 <- numeric(0)
index <- 1:N
index2<-(burn+1):N
X21 <- gibbs_bivariate(N,d11,d21)$X[ ,1]  #X sample 
Y21 <- gibbs_bivariate(N,d11,d21)$X[ ,2]  #Y sample 

#par(mfrow=c(1,2))
plot(index, X21, type='l', ylab='Values of X', xlab='Iterations', main='(a) Trace Plot of X')
plot(index, Y21, type='l', ylab='Values of Y', xlab='Iterations', main='(b) Trace Plot of Y')

#Comparison for ergodic mean plot 
erg.mean<-function(x){ 
  n<-length(x)
  result<-cumsum(x)/cumsum(rep(1,n))
}
   
ergtheta0<-erg.mean(X21)
ergtheta02<-erg.mean(X11)
ylims0<-range(c(ergtheta0,ergtheta02))

ergtheta1<-erg.mean(Y21)
ergtheta12<-erg.mean(Y11)
ylims1<-range(c(ergtheta1,ergtheta12))

step<-10
index3<-seq(1,N,step)
index4<-seq(burn+1,N,step)

plot(index3 , ergtheta0[index3], type='l', ylab='Values of X', xlab='Iterations', main='(c) Ergodic Mean Plot of X', ylim=ylims0)
lines(index4, ergtheta02[index4-burn], col=2, lty=2)

plot(index3, ergtheta1[index3], type='l', ylab='Values of Y', xlab='Iterations', main='(d) Ergodic Mean Plot of Y', ylim=ylims1)
lines(index4, ergtheta12[index4-burn], col=2, lty=2)

#Comparison for acf plot
acf(X11, main='Autocorrelations Plot for X')
acf(Y11, main='Autocorrelations Plot for Y')
```

***Step 4:*** Use the Gelman-Rubin method to monitor convergence of the chain.
```{r}
#generate the chains
set.seed(64)
X12 <- Y12 <- X22 <- Y22 <- X13 <- Y13 <- X23 <- Y23 <- numeric(0)
X12 <- gibbs_bivariate(N,d12,d22)$x[ ,1]  #X sample after remove the burn-in sample
Y12 <- gibbs_bivariate(N,d12,d22)$x[ ,2]  #Y sample after remove the burn-in sample
X22 <- gibbs_bivariate(N,d12,d22)$X[ ,1]  #X sample 
Y22 <- gibbs_bivariate(N,d12,d22)$X[ ,2]  #Y sample 
X13 <- gibbs_bivariate(N,d13,d23)$x[ ,1]  #X sample after remove the burn-in sample
Y13 <- gibbs_bivariate(N,d13,d23)$x[ ,2]  #Y sample after remove the burn-in sample
X23 <- gibbs_bivariate(N,d13,d23)$X[ ,1]  #X sample 
Y23 <- gibbs_bivariate(N,d13,d23)$X[ ,2]  #Y sample 

#compute diagnostic statistics
XX <- rbind(X11,X12,X13)
psi4 <- t(apply(XX, 1, cumsum))
for (i in 1:nrow(psi4))
  psi4[i,] <- psi4[i,] / (1:ncol(psi4))

YY <- rbind(Y11,Y12,Y13)
psi5 <- t(apply(YY, 1, cumsum))
for (i in 1:nrow(psi5))
  psi5[i,] <- psi5[i,] / (1:ncol(psi5))

#plot the sequence of R-hat statistics
#par(mfrow = c(1, 2))
rhat4 <- rep(0, ncol(psi4))
for (j in 1:ncol(psi4))
  rhat4[j] <- Gelman.Rubin(psi4[,1:j])
plot(rhat4[1:ncol(psi4)], type="l", xlab="X", ylab="R")
abline(h=1.2, lty=2)

rhat5 <- rep(0, ncol(psi5))
for (j in 1:ncol(psi5))
  rhat5[j] <- Gelman.Rubin(psi5[,1:j])
plot(rhat5[1:ncol(psi5)], type="l", xlab="Y", ylab="R")
abline(h=1.2, lty=2)
```

**Conclusion.** It is not difficult to draw the following conclusions:

(1). The scatter plot of the generated chain shows that it has elliptical symmetry and is located at the origin of the bivariate normal distribution. And the generated bivariate samples $\small(X_1,Y_1)$ has highly linear correlation relationship.

(2).The coefficients of the fitting model match well with the parameters of the target distribution. We can express the regression equation as follows$$\small Y=-0.07267-0.02386X.$$

(3). The residual plot suggests that the error variance is constant relative to the response variable. The constant variance is 0.9 Which is because: if $\small Y=0.9X$ and $\small Var(X)=Var(Y)=1$, then $\small Corr(X,Y)=Corr(X,0.9X)=0.9$. The Q-Q plot of residuals vs. fits suggests that the residuals obeys the normal distribution.

(4). It can be seen from the ergodic mean graph and the $\small \hat{R}$-graph generated by Gelman-Rubin method that the chain has good convergence.

## 2022-11-04

## Question

1. Exercise1. 

2. Exercise2. 

## Answer

### Exercise1 
 **Problem.** Check the mediating effect question as follows
 $$\small H_0:\alpha\beta=0  \quad \leftrightarrow \quad H_1:\alpha\beta \neq 0$$
 in which the test statistics is 
 $$\small T=\displaystyle\frac{\hat{\alpha}\hat{\beta}}{\hat{se}(\hat{\alpha}\hat{\beta})}$$
and the rejection region is $\small \{|T|>\tau\}$. There are three permutation test methods
$$
\small
\begin{aligned}
&(1). \alpha=0 \; (X \perp M); \\
&(2). \beta=0 \; (M \perp Y); \\
&(3). \alpha=0 \;\& \;\beta=0 \; (X \perp M \; \& \; Y \perp M). \\
\end{aligned}
$$
Now set up a random simulation study to examine the performance of the above three permutation test methods. Consider model
$$
\small
\begin{aligned}
M&=a_M+\alpha X +e_M \\
Y&=a_Y+\beta M +\gamma X+e_Y
\end{aligned}
$$
in which $\small e_M,e_Y \stackrel{i.i.d}{\sim} N(0,1).$ The following three parameter combinations need to be considered to proof (1),(2) and (3) by using permutation.
$$
\small
\begin{aligned}
&(a). \alpha=0 , \beta=0, \gamma=1; \\
&(b). \alpha=0 , \beta=1, \gamma=1;\\
&(c). \alpha=1 , \beta=0, \gamma=1.\\
\end{aligned}
$$


**Solution.** Assume that $\small X \sim N(0,1)$ and the parameters $\small a_M=a_Y=0$, then generate the value of $\small M$ and $\small Y$ by the above three different parameter combinations and formulas. The R package named **multilevel** and **bda** will be used to output the mediating effect results generating by Sobel test, in which the test statistics formula will be shown as follows
$$\small T=\displaystyle\frac{\hat{\alpha}\hat{\beta}}{\hat{se}(\hat{\alpha}\hat{\beta})}=\displaystyle\frac{\hat{\alpha}\hat{\beta}}{\sqrt{\hat{\alpha}^2{se_{\hat{\beta}}}^2+\hat{\beta}^2{se_{\hat{\alpha}}}^2}}.$$
The solution will be divided into the following three steps.

***Step 1: *** For the parameter combination (a), the permutation will be used for the indexes of $\small X$ and $\small Y$ which is not break the within-x and within-y structures. The code will be shown as follows.
```{r}
rm(list=ls())                 #clear variables
set.seed(578)
library(multilevel)
library(bda)
### (1). Data generation
alpha <- beta <- 0; gamma <-1; a_M <- a_Y <- 0;               #known parameters
X <- rnorm(80,0,1);e_M <- rnorm(80,0,1);e_Y <- rnorm(80,0,1)  #generation of random number
# generation of M&Y
M_s <- function(a,b){
  a+b*X+e_M
}
Y_s <- function(a,b,c,M){
  a+b*M+c*X+e_Y
}
MM1 <- M_s(a_M,alpha) 
YY1 <- Y_s(a_Y,beta,gamma,MM1)
# test for mediating effect
solution1 <- sobel(pred=X, med=MM1, out=YY1) #results
T1_orignal <- solution1$z.value              #statistics 
p1_val <- mediation.test(MM1,X,YY1)[2,1]     #p-value

### (2). Data analysis
R <- 999              #number of replicates
K <- 1:length(X)
#permutation of X&Y&M
permu <- function(X,Y,M){
  T1_per <- numeric(R)   #storage for replicates
  for (i in 1:R) {
  #generate indices k for the first sample
  k1 <- sample(K, size = length(X), replace = FALSE)
  X1 <- X[k1]
  k2 <- sample(K, size = length(Y), replace = FALSE)
  Y1 <- Y[k2] 
  res <- sobel(pred=X1, med=M, out=Y1)
  T1_per[i] <- res$z.value
  }
  return(T1_per)
}

### (3). Result reporting
p1 <- mean(abs(c(T1_orignal,permu(X,YY1,MM1))) > abs(T1_orignal))  #compute p-value
result <- function(p_before,p_after){
  round(c(p_before,p_after),3)         #before&after
}
result1 <- result(p1_val,p1)
hist(permu(X,YY1,MM1),nclass="scott",xlab="",main="",freq=FALSE)
abline(v=T1_orignal,col="red",lwd=2)
```

***Step 2: *** For the parameter combination (b), the permutation will be used for the indexes of $\small X$ which is not break the within-x structure. The code will be shown as follows.
```{r}
### (1). Data generation
alpha <-0 ;beta <- 1;      #Known parameters
# generation of M&Y
M_s <- function(a,b){
  a+b*X+e_M
}
Y_s <- function(a,b,c,M){
  a+b*M+c*X+e_Y
}
MM2 <- M_s(a_M,alpha) 
YY2 <- Y_s(a_Y,beta,gamma,MM2)
# test for mediating effect
solution2 <- sobel(pred=X, med=MM2, out=YY2) #results
T2_orignal <- solution2$z.value              #statistics 
p2_val <- mediation.test(MM2,X,YY2)[2,1]     #p-value

### (2). Data analysis
R <- 999               #number of replicates
K <- 1:length(X)
T2_per <- numeric(R)   #storage for replicates
#permutation of X&Y&M
permu <- function(X,Y,M){
  for (i in 1:R) {
  #generate indices k for the first sample
  z <- c(X,M) #pooled sample
  k <- sample(K, size = length(X), replace = FALSE)
  X1 <- X[k]
  res <- sobel(pred=X1, med=M, out=Y)
  T2_per[i] <- res$z.value
  }
  return(T2_per)
}

### (3). Result reporting
p2 <- mean(abs(c(T2_orignal,permu(X,YY2,MM2))) > abs(T2_orignal))  #compute p-value
result <- function(p_before,p_after){
  round(c(p_before,p_after),3)         #before&after
}
result2 <- result(p2_val,p2)
hist(permu(X,YY2,MM2),nclass="scott",xlab="",main="",freq=FALSE)
abline(v=T2_orignal,col="red",lwd=2)
```

***Step 3: *** For the parameter combination (c), the permutation will be used for the indexes of $\small Y$ which is not break the within-y structure. The code will be shown as follows.
```{r}
### (1). Data generation
alpha <-1 ;beta <- 0;      #Known parameters
# generation of M&Y
M_s <- function(a,b){
  a+b*X+e_M
}
Y_s <- function(a,b,c,M){
  a+b*M+c*X+e_Y
}
MM3 <- M_s(a_M,alpha) 
YY3 <- Y_s(a_Y,beta,gamma,MM3)
# test for mediating effect
solution3 <- sobel(pred=X, med=MM3, out=YY3) #results
T3_orignal <- solution3$z.value              #statistics 
p3_val <- mediation.test(MM3,X,YY3)[2,1]     #p-value

### (2). Data analysis
R <- 999               #number of replicates
K <- 1:length(X)
T3_per <- numeric(R)   #storage for replicates
#permutation of X&Y&M
permu <- function(X,Y,M){
  for (i in 1:R) {
  #generate indices k for the first sample
  k <- sample(K, size = length(X), replace = FALSE)
  Y1 <- Y[k]
  res <- sobel(pred=X, med=M, out=Y1)
  T3_per[i] <- res$z.value
  }
  return(T3_per)
}

### (3). Result reporting
p3 <- mean(abs(c(T3_orignal,permu(X,YY3,MM3))) > abs(T3_orignal))  #compute p-value
result <- function(p_before,p_after){
  round(c(p_before,p_after),3)         #before&after
}
result3 <- result(p3_val,p3)
hist(permu(X,YY3,MM3),nclass="scott",xlab="",main="",freq=FALSE)
abline(v=T3_orignal,col="red",lwd=2)

### (4). Total conclusion
df <- data.frame(result1,result2,result3)
row.names(df) <- c("before permutation","after permutation")
df

detach(package:multilevel)    #Uninstall R Package
detach(package:bda)    
```

**Conclusion.** It is not difficult to see that the in these three models, the p-value before the permutation is larger than the p-value after the permutation, which indicates that these three permutation methods cannot control the Type I errors well.

### Exercise 

**Problem.** Consider model
$$\small P(Y=1|X_1,X_2,X_3)=expit(a+b_1X_1+b_2X_2+b_3X_3).$$
in which $\small X_1 \sim P(1),X_2 \sim Exp(1),X_3 \sim B(1,0.5).$ Solve the following questions.

(1). Write an R function to achieve the above functions in which the input values are $\small N,b_1,b_2,b_3,f_0$ and output value is $\small \alpha.$

(2). Call this function, and the input value is $\small N=10^6,b_1=0,b_2=1,b_3=-1,f_0=0.1,0.01,0.001,0.0001.$

(3). Draw a scatter plot $\small f_0 \; vs. \; \alpha.$

**Solution.** The solution will be divided into the following three steps.

***Step 1: *** The code for R function **alpha1-alpha4** in which the input values are $\small N,b_1,b_2,b_3,f_0$ and output value is $\small \alpha.$.

```{r}
rm(list=ls())                 #clear variables
### (1). Data analysis
alpha1 <- function(N,b1,b2,b3,f0){
  solution1 <- uniroot(g1,c(-10,0))
  return(solution1$root)
}

alpha2 <- function(N,b1,b2,b3,f0){
  solution2 <- uniroot(g2,c(-10,0))
  return(solution2$root)
}

alpha3 <- function(N,b1,b2,b3,f0){
  solution3 <- uniroot(g3,c(-10,0))
  return(solution3$root)
}

alpha4 <- function(N,b1,b2,b3,f0){
  solution4 <- uniroot(g4,c(-20,0))
  return(solution4$root)
}
```

***Step 2: *** Call the above R function.

```{r}
### (2). Data generation
set.seed(12345)
N <- 1e6; b1 <- 0; b2 <- 1; b3 <- -1; 
f01 <- 0.1; f02 <- 0.01; f03 <- 0.001; f04 <-0.0001;
f0 <- c(f01,f02,f03,f04);
x1 <- rpois(N,1); x2 <- rexp(N,1); x3 <- rbinom (N,1,0.5)
g1 <- function(alpha){
  tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
  p <- 1/(1+tmp)
  mean(p) - f01
}

g2 <- function(alpha){
  tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
  p <- 1/(1+tmp)
  mean(p) - f02
}

g3 <- function(alpha){
  tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
  p <- 1/(1+tmp)
  mean(p) - f03
}

g4 <- function(alpha){
  tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3)
  p <- 1/(1+tmp)
  mean(p) - f04
}

### (3). Result reporting
solve <- function(N,b1,b2,b3){
  solution1 <- alpha1(N,b1,b2,b3,f01)
  solution2 <- alpha2(N,b1,b2,b3,f02)
  solution3 <- alpha3(N,b1,b2,b3,f03)
  solution4 <- alpha4(N,b1,b2,b3,f04)
  alpha_s <- c(solution1,solution2,solution3,solution4)
  return(alpha_s)
}
```

***Step 3: *** Draw a scatter plot for $\small -log(f_0)$ and $\small \alpha.$
```{r}
plot(-log(f0),solve(N,b1,b2,b3), type="p")
```

**Conclusion.** It is not difficult to see that $\small \alpha$ decreases with the increase of $\small -log(f_0)$.


## 2022-11-11

## Question

1. Exercise 1. 

2. Exercise 4, 5 (Pages 19 Advanced in R) 

3. Exercise 1, 2 (Pages 26 Advanced in R) 

4. Exercise 1, 2, 3 (Pages 30 Advanced in R) 

## Answer

### Exercise 1 
 **Problem.** Let $\small X_1, X_2,\cdots,X_n$ be the independent identically distributed samples from $\small Exp(\lambda)$. For some reason, we only know that $\small X_i$ falls between a certain interval $\small (u_i, v_i)$, in which $\small u_i < v_i$ is two known non random constants. Such data is called interval deletion data. Please solve the following problems:
 
(1). Try to maximize the likelihood function of observed data directly and solve the maximum likelihood estimation of $\small \lambda$ using EM algorithm respectively, then prove that they are equal.

(2). Let ten observed data of $\small (u_i,v_i),i=1,2,\cdots,10$ be (11,12),(8,9),(27,28),(13,14),(16,17),(0,1),(23,24),(10,11),(24,25),(2,3). Try to program the above two algorithms respectively to obtain the numerical solution of the maximum likelihood estimation of $\small lambda.$

Prompt: the likelihood function of observation data is $\small L(\lambda)=\prod_{i=1}^{n} P_\lambda (u_i \leq X_i \leq v_i).$

**Solution.**

(1). **I.Maximize the likelihood function of observed data.**

The maximum likelihood function of observed data is
$$\small
\begin{aligned}
L(\lambda)&=\prod_{i=1}^{n} P_\lambda(u_i \leq X_i \leq v_i) \\
&=\prod_{i=1}^{n} F(v_i)-F(u_i) \\
&=\prod_{i=1}^{n} e^{-\lambda u_i}-e^{-\lambda v_i}.
\end{aligned}
$$
Then, the log-maximum likelihood is
$$\small lnL(\lambda)=\sum_{i=1}^{n} ln(e^{-\lambda u_i}-e^{-\lambda v_i}).$$
The derivation of the above equation on $\small \lambda$ is 
$$\small \displaystyle\frac{dln L(\lambda)}{d\lambda}=\sum_{i=1}^{n}\displaystyle\frac{-u_ie^{-\lambda u_i}+v_ie^{-\lambda v_i}}{e^{-\lambda u_i}-e^{\lambda v_i}}.$$
Let the above equation be zero so that $\small \hat{\lambda}_{MLE}$ can be found.

**II. using EM algorithm to solve the maximum likelihood estimation of $\small \lambda$**

The likelihood function of EM algorithm is
$$\small L_c(\lambda)=\prod_{i=1}^{n}f_\lambda(x_i)=\prod_{i=1}^{n}\lambda e^{-\lambda x_i}=\lambda^n e^{-\lambda \sum_{i=1}^{n}x_i}.$$
Then, the log-maximum likelihood is
$$\small lnL_c(\lambda)=nln\lambda-\lambda \sum_{i=1}^{n}x_i.$$
Then for **E step**, we have
$$\small E_{\hat{\lambda}_0}[lnL_c(\lambda)|x_i \in[u_i,v_i],i=1,\cdots,n]=nln\lambda-\lambda\sum_{i=1}^{n} E_{\hat{\lambda}_0}[x_i|x_i \in[u_i,v_i],i=1,\cdots,n].$$
Let $\small \hat{x_i}^{(0)}=E_{\hat{\lambda}_0}[x_i|x_i \in[u_i,v_i],i=1,\cdots,n].$ The partial derivation of the above equation on $\small \lambda$ is 

$$\small \displaystyle\frac{\partial lnL(\lambda,\hat{\lambda}_0)}{\partial \lambda} = \displaystyle\frac{n}{\lambda}-\sum_{i=1}^{n} \hat{x_i}^{(0)}.$$
Let the above equation be zero so that $\small \hat{\lambda}=\displaystyle\frac{n}{\sum_{i=1}^{n} \hat{x_i}^{(0)}}$.

Then calculate conditional expectation $\small \hat{x_i}^{(0)}.$ Actually,we have

$$\small E[x_i|x_i \in [0,u_i)]=\displaystyle\frac{E[x]-E[x_i|x_i \in [u_i,+\infty)]P(x_i \in [u_i,+\infty))}{P(x_i \in [0,u_i))}.$$
So

$$
\small
\begin{aligned}
E[x_i|x_i \in [u_i,v_i]]&=\displaystyle\frac{E[x]-E[x_i|x_i \in [0,u_i)]P(x_i \in [0,u_i))-E[x_i|x_i \in (v_i,+\infty)]P(x_i \in (v_i,+\infty))}{P(x_i \in [u_i,v_i])} \\
&=\displaystyle\frac{E[x_i|x_i \in [u_i,+\infty)]P(x_i \in [u_i,+\infty))-E[x_i|x_i \in (v_i,+\infty)]P(x_i \in (v_i,+\infty))}{P(x_i \in [u_i,v_i])} \\
&=\displaystyle\frac{\left(\displaystyle\frac{1}{\lambda^{(0)}}+u_i\right)e^{-\lambda^{(0)}u_i}-\left(\displaystyle\frac{1}{\lambda^{(0)}}+v_i\right)e^{-\lambda^{(0)}v_i}}{e^{-\lambda^{(0)}u_i}-e^{-\lambda^{(0)}v_i}} \\
&=\displaystyle\frac{u_i e^{-\lambda^{(0)}u_i}-v_i e^{-\lambda^{(0)}v_i}}{e^{-\lambda^{(0)}u_i}-e^{-\lambda^{(0)v_i}}}+\displaystyle\frac{1}{\lambda^{(0)}}.
\end{aligned}
$$

Then for **M step**, we have 
$$\small \lambda^{(1)}=\displaystyle\frac{n}{\displaystyle\sum_{i=1}^{n}\left[\displaystyle\frac{u_i e^{-\lambda^{(0)}u_i}-v_i e^{-\lambda^{(0)}v_i}}{e^{-\lambda^{(0)}u_i}-e^{-\lambda^{(0)v_i}}}+\displaystyle\frac{1}{\lambda^{(0)}}\right]}.$$
Therefore, the recurrence formula is
$$\small \lambda^{(t+1)}=\displaystyle\frac{n}{\displaystyle\sum_{i=1}^{n}\left[\displaystyle\frac{u_i e^{-\lambda^{(t)}u_i}-v_i e^{-\lambda^{(t)}v_i}}{e^{-\lambda^{(t)}u_i}-e^{-\lambda^{(t)v_i}}}+\displaystyle\frac{1}{\lambda^{(t)}}\right]}.$$
Under some mild conditions, $\small \lambda^{(t+1)}$ converges to the MLE for the observed data (Wu,1983). The converged $\small \lambda$ estimate is the solution of the equation

$$\small \sum_{i=1}^{n} \displaystyle\frac{u_i e^{-\lambda^{(t)}u_i}-v_i e^{-\lambda^{(t)}v_i}}{e^{-\lambda^{(t)}u_i}-e^{-\lambda^{(t)v_i}}} = 0.$$
Thus, the EM estimate is exactly the observed data MLE.

(2). The code for these two methods are shown as follows.

```{r} 
rm(list=ls())                 #clear variables
### (1). Data generation
u <- c(11,8,27,13,16,0,23,10,24,2)
v <- c(12,9,28,14,17,1,24,11,25,3) 

### (2). Data analysis
#Method one
comp <- function(x){
  s11 <- 0
  for (i in 1:length(u)){
    s11 <- s11 + (-u[i]*exp(-x*u[i])+v[i]*exp(-x*v[i]))/(exp(-x*u[i])-exp(-x*v[i]))
  }
 return(s11)
}

#Method two
EM <- function(u,v,max.it=5000,eps=1e-6){
  lam <- 0.072
  i <- 1
  lam1 <- 0.8
  lam2 <- 0.2
  s12 <- 0
  for (j in 1:length(u)){
    s12 <- s12 + (u[j]*exp(-lam*u[j])-v[j]*exp(-lam*v[j]))/(exp(-lam*u[j])-exp(-lam*v[j]))
  } 
  s22 <- s12 + length(u)/lam  #分母
  while( abs(lam1 - lam2) >= eps){
    lam1 <- lam2
    lam2 <- length(u)/s22
    for (j in 1:length(u)){
      s12 <- s12 + (u[j]*exp(-lam2*u[j])-v[j]*exp(-lam2*v[j]))/(exp(-lam2*u[j])-exp(-lam2*v[j]))
    } 
    s22 <- s12 + length(u)/lam2
    if(i == max.it) break
    i <- i + 1    
  }
  return(lam2)
}

### (3). Result reporting
curve(comp,0,0.5)       #Determine dichotomy interval
abline(h = 0,lty = 3)
root1 <- uniroot(comp,c(0,0.7),tol = 0.00001)
root2 <- EM(u,v,max.it=5000,eps=1e-6)
result <- function(root1,root2){
  return(list(MLE=root1$root,EM=root2))
}
result(root1,root2)
```

It is not difficult to see that the numerical solution of $\small \lambda$'s MLE obtained by the two algorithms is very close.

### Exercise 4,5 (2.1.3)

**Problem.** 

4. Why do you need to use **unlist()** to convert a list to an atomic vector? Why doesn’t **as.vector()** work?

5. Why is **1 == "1"** true? Why is **-1 < FALSE** true? Why is **"one" < 2** false?

**Solution.** 

4. The reason why we need to use **unlist()** to convert a list to an atomic vector rather than **as.vector()** is to get rid of (flatten) the nested structure. Actually, a list is already a vector, though not an atomic one and we could use **unlist()** or **is.vector()** to convert it. The following example can prove our point.

```{r}
rm(list=ls())                 #clear variables
l <- list( name="Joe", salary=55000, union=T )
is.atomic(unlist(l))
is.atomic(as.vector(l))
is.atomic(is.vector(l))
```

5. These operators are functions that force their arguments (in these cases) to characters, doubles, and characters. In detail, in Example 1, the **1** will be forced to be **"1"**, so we have **"1"="1"**, and the output result is **TRUE**. In example2, the **FALSE** will be represented to be **0**, so we have **-1<0** which is definitely **TRUE**. In example 3, the **2** will be forced to be **"2"**, so we have **"one"<"2"**. Because **"one"** comes after **"2"** in ASCII, so we have the output result **FALSE**.

```{r}
rm(list=ls())                 #clear variables
1 == "1"
-1 < FALSE
"one" < 2
```

### Exercise 1,2 (2.3.1) 

**Problem.**

1. What does **dim()** return when applied to a vector?

2. If **is.matrix(x)** is TRUE, what will **is.array(x)** return?

**Solution.** 

1. We can get the answer by **?nrow**, it is said that **dim()** will return **NULL** when applied to a $\small 1 \times d$ vector. The following example demonstrates this point.

```{r}
rm(list=ls())                 #clear variables
v1<-c(1,5,8,6,0) 
dim(v1)
```

2. **is.array(x)** will return **TRUE** which can be demonstrated by the following example. In addition, We can find such a sentence "A two-dimensional array is the same thing as a matrix." by calling command **?array** which proves the correctness of the example.

```{r}
rm(list=ls()) 
A <- matrix(c(1:6),nrow=2,ncol=3,dimnames=list(c("r1","r2"),c("c1","c2","c3")))
is.matrix(A)
is.array(A)
```

### Exercise 1, 2, 3 (2.4.5) 

**Problem.** 

1. What attributes does a data frame possess?

2. What does **as.matrix()** do when applied to a data frame with columns of different types?

3. Can you have a data frame with zero rows? What about zero columns?

**Solution.** 

1. Its properties include names, row.names and class.

2. Actually,the types of the input columns decides the type of the result of **as.matrix** and we can see **?as.matrix** for more details.

**as.matrix** is a generic function in base R. It dispatches to **as.matrix.data.table** if its x argument is a **data.table**. The method for **data.table**s will return a character matrix if there are only atomic columns and any non-(numeric/logical/complex) column, applying **as.vector** to factors and **format** to other non-character columns. Otherwise, the usual coercion hierarchy (logical < integer < double < complex) will be used, e.g., all-logical data frames will be coerced to a logical matrix, mixed logical-integer will give an integer matrix, etc.

3. Yes, a data frame with zero rows or zero columns or with both can be easily created. Consider the following example.

```{r}
rm(list=ls()) 
#data frame with 2 columns and 0 rows
data.frame(f = logical(), g = integer())

#data frame with 0 columns and 3 rows
data.frame(row.names = 1:5)  

#data frame with 0 columns and 0 rows
data.frame()
```

## 2022-11-18

## Question

1. Exercise 2 (page 204, Advanced R) 

2. Exercise 1 (page 213, Advanced R) 

3. Exercise 3 (Pages 1, HW10) 

## Answer

### Exercise 2 

**Problem.** The function below scales a vector so it falls in the range [0,1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame?

```{r echo = T, results = 'hide'}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
```

**Solution.** The **if-else** clause can be used to check the input part of the function. Use function **is.numeric()** to check whether the input part is a numerical value. If it is, use function **scale01** to scale it so that it can fall in the range [0,1]. Otherwise, the original non-numeric column will be returned. The data set **chickwts** in R will be called as an example. The code can be shown as follows.

```{r}
check_data <- function(y){
  if (is.numeric(y))
    scale01(y)
  else y
}
data.frame(lapply(chickwts,check_data))[1:10,] #Top ten lines
#data.frame(lapply(chickwts,check_data))       #complete result
rm(list=ls())                                  #clear variables
```

### Exercise 1

**Problem.** Use **vapply()** to:

a). Compute the standard deviation of every column in a numeric data frame.

b). Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use **vapply()** twice.)

**Solution.**

a). The data set **faithful** in R is used to calculate the standard deviation of each column. Here the argument named **FUN VALUE** is set to **numeric(1)**, which means that we only need a single value, so the output result is a data frame list as follows.

```{r}
rm(list=ls())                 #clear variables
vapply(faithful,sd,numeric(1))
```

We can see that the standard deviation of column **eruptions** and column **waiting** are 1.141371 and 13.594974 respectively.

b). Firstly, a **vapply** function is used to check whether the input column is of numerical type. The argument **FUN.VALUE** is set to **logical(1)**. If it is, **TRUE** is returned. Otherwise, **FALSE** is returned. Then only the columns with the result of **TRUE** are called, and another **vapply** function is used to return the standard deviation of each numerical column. The data set **chickwts** in R will be called as an example. The code can be shown as follows.

```{r}
rm(list=ls())                 #clear variables
vapply(chickwts[vapply(chickwts, is.numeric, logical(1))],sd, numeric(1))
```

We can see that the standard deviation of the numerical column **weight** is 78.0737.

### Exercise 3 

**Problem.** Implement a Gibbs sampler to generate a bivariate normal chain $\small (X_t,Y_t)$ with zero means, unit standard deviations, and correlation 0.9.

(1). Write an Rcpp function.

(2). Compare the corresponding generated random numbers with pure R language using the function **qqplot**.

(3). Compare the computation time of the two functions with the function **microbenchmark**.

**Solution.**

(1). First of all, we know that in the case of bivariate normal, $\small X_1|X_2$ and $\small X_2|X_1$ still obey the normal distribution, that is
$$\small
\begin{aligned}
f(x_1|x_2) & \sim N\left(\mu_1+\rho\displaystyle\frac{\sigma_1}{\sigma_2}(x_2-\mu_2),(1-\rho^2)\sigma_1^2\right), \\
f(x_2|x_1) & \sim N\left(\mu_2+\rho\displaystyle\frac{\sigma_2}{\sigma_1}(x_1-\mu_1),(1-\rho^2)\sigma_2^2\right). \\
\end{aligned}$$

Thus,the Rcpp function for a Gibbs sampler can be shown as follows.

```{r}
rm(list=ls())                 #clear variables
library(Rcpp)
cppFunction('NumericMatrix gibbsC (int N) {
  NumericMatrix mat (N, 2);
  double rho = 0.9, mu1 = 0, mu2 = 0, sigma1 = 1, sigma2 = 1;
  double s1 = sqrt(1-pow(rho,2))*sigma1;
  double s2 = sqrt(1-pow(rho,2))*sigma2;
  double x = mu1, y = mu2;
  for (int i=0; i<N; i++) {
    double m1 = mu1 + rho * (y - mu2) * sigma1/sigma2;
    x = rnorm(1, m1, s1)[0];
    double m2 = mu2 + rho * (x - mu1) * sigma2/sigma1;
    y = rnorm(1, m2, s2)[0];
    mat(i,0) = x;
    mat(i,1) = y;
  }
  return(mat);
}')
```

(2). We use the function **qqplot** to compare whether the random numbers generated by the pure R language function **gibbsR** and the pure C++ language function **gibbsR** obey the same distribution. The code can be shown as follows.

```{r}
gibbsR <- function(N) {
  mat <- matrix(0, N, 2)
  rho <- 0.9             #correlation
  mu1 <- mu2 <- 0
  sigma1 <- sigma2 <- 1
  s1 <- sqrt(1-rho^2)*sigma1
  s2 <- sqrt(1-rho^2)*sigma2
  mat[1, ] <- c(mu1, mu2)            #initialize
  for (i in 2:N) {
    x2 <- mat[i-1, 2]
    m1 <- mu1 + rho * (x2 - mu2) * sigma1/sigma2
    mat[i, 1] <- rnorm(1, m1, s1)
    x1 <- mat[i, 1]
    m2 <- mu2 + rho * (x1 - mu1) * sigma2/sigma1
    mat[i, 2] <- rnorm(1, m2, s2)
  }
  mat
}
```

```{r}
gc <- gibbsC(1000)
gr <- gibbsR(1000)
qqplot(gc,gr)
qqplot(gc[,1],gr[,1])
qqplot(gc[,2],gr[,2])
```

It can be seen from the results of the Q-Q diagrams that the random numbers generated by these two generators follow the same distribution.

(3). We use the function **microbenchmark** in the R package **microbenchmark** to compare the efficiency of these two generators. The code can be shown as follows.

```{r}
library(microbenchmark)
ts <- microbenchmark(gibbR=gibbsR(1000),gibbC=gibbsC(1000))
summary(ts)[,c(1,3,5,6)]
detach("package:microbenchmark")
rm(list=ls()) 
```
From the above summary results, we can see that the efficiency of the C++ generator **gibbC** is better than that of the R generator **gibbR** on the whole. (the running time of the R generator **gibbR** is 10-30 times that of the C++ generator **gibbC**.)

**This is the end of the document. Thanks for your reading and suggestions!**